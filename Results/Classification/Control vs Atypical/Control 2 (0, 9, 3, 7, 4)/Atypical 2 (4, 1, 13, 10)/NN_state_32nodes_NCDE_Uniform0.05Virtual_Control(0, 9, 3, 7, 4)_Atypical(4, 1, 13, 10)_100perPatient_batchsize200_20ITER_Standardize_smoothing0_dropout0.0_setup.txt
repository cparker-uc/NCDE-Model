Model Setup for Uniformvirtual {PATIENT_GROUPS} Trained Network:

NCDE Network Architecture Parameters
Input channels=3
Hidden channels=32
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=20
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control', 'Atypical']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=Standardize
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: (0, 9, 3, 7, 4)
MDD: (4, 1, 13, 10)
Training batch size=200
Training Results:
Runtime=450.28137016296387
Loss over time=[(0, 0.6467348337173462), (1, 0.6428205966949463), (2, 0.6094564199447632), (3, 0.6038569808006287), (4, 0.6239425539970398), (5, 0.5818279385566711), (6, 0.5719340443611145), (7, 0.55877286195755), (8, 0.5699566006660461), (9, 0.5599064826965332), (0, 0.5616463422775269), (1, 0.5163748264312744), (2, 0.524172842502594), (3, 0.5166828036308289), (4, 0.515800416469574), (5, 0.5270940065383911), (6, 0.5778558850288391), (7, 0.481806218624115), (8, 0.5072014331817627), (9, 0.5010837912559509), (0, 0.46351128816604614), (1, 0.4805089831352234), (2, 0.474741131067276), (3, 0.5403782725334167), (4, 0.4760737717151642), (5, 0.4552363157272339), (6, 0.4661877453327179), (7, 0.44527095556259155), (8, 0.4780842959880829), (9, 0.47422316670417786), (0, 0.4416714906692505), (1, 0.4369429647922516), (2, 0.4491775929927826), (3, 0.43328461050987244), (4, 0.3930467963218689), (5, 0.4593524932861328), (6, 0.40207186341285706), (7, 0.43439576029777527), (8, 0.3736327290534973), (9, 0.3844297528266907), (0, 0.46283358335494995), (1, 0.39523616433143616), (2, 0.4119187891483307), (3, 0.37019044160842896), (4, 0.3555879294872284), (5, 0.3644580841064453), (6, 0.3485599160194397), (7, 0.3349998891353607), (8, 0.36696159839630127), (9, 0.36779940128326416), (0, 0.29375725984573364), (1, 0.34602200984954834), (2, 0.3653427064418793), (3, 0.3567308783531189), (4, 0.31393560767173767), (5, 0.31047874689102173), (6, 0.2978779077529907), (7, 0.348428875207901), (8, 0.2861175835132599), (9, 0.30955636501312256), (0, 0.3447735607624054), (1, 0.31844374537467957), (2, 0.25784218311309814), (3, 0.35119178891181946), (4, 0.2703690826892853), (5, 0.24134518206119537), (6, 0.26833027601242065), (7, 0.22032789885997772), (8, 0.21471451222896576), (9, 0.22640489041805267), (0, 0.277631938457489), (1, 0.20896205306053162), (2, 0.20276623964309692), (3, 0.2267189770936966), (4, 0.19704212248325348), (5, 0.17950284481048584), (6, 0.14883096516132355), (7, 0.22975361347198486), (8, 0.12630783021450043), (9, 0.1762547641992569), (0, 0.1495383232831955), (1, 0.15943492949008942), (2, 0.1300550252199173), (3, 0.13544510304927826), (4, 0.08599113672971725), (5, 0.11080911010503769), (6, 0.1313685029745102), (7, 0.11273657530546188), (8, 0.11972260475158691), (9, 0.11024090647697449), (0, 0.20743457973003387), (1, 0.16357900202274323), (2, 0.12424767762422562), (3, 0.09371168166399002), (4, 0.10125861316919327), (5, 0.14369717240333557), (6, 0.09561396390199661), (7, 0.10789293050765991), (8, 0.11983608454465866), (9, 0.058526478707790375), (0, 0.19096118211746216), (1, 0.09322643280029297), (2, 0.07559017837047577), (3, 0.09822855144739151), (4, 0.09626096487045288), (5, 0.0776444524526596), (6, 0.05301468446850777), (7, 0.07873229682445526), (8, 0.07784958183765411), (9, 0.06301894038915634), (0, 0.09875092655420303), (1, 0.07488547265529633), (2, 0.09076422452926636), (3, 0.05587078258395195), (4, 0.04602101817727089), (5, 0.06190114468336105), (6, 0.05059121176600456), (7, 0.04011385142803192), (8, 0.0501738041639328), (9, 0.057168468832969666), (0, 0.037914324551820755), (1, 0.054579898715019226), (2, 0.042487479746341705), (3, 0.05055005103349686), (4, 0.07867807894945145), (5, 0.052979107946157455), (6, 0.052965860813856125), (7, 0.04493657127022743), (8, 0.05790306255221367), (9, 0.03960298374295235), (0, 0.09595268219709396), (1, 0.04344750568270683), (2, 0.06371185183525085), (3, 0.027271516621112823), (4, 0.04508373141288757), (5, 0.05099930241703987), (6, 0.0331256128847599), (7, 0.13507714867591858), (8, 0.027118127793073654), (9, 0.07858994603157043), (0, 0.038944046944379807), (1, 0.027478942647576332), (2, 0.06537093967199326), (3, 0.051597725600004196), (4, 0.024990402162075043), (5, 0.027673304080963135), (6, 0.05677191913127899), (7, 0.02315630204975605), (8, 0.17344123125076294), (9, 0.051487162709236145), (0, 0.10521963238716125), (1, 0.0684150755405426), (2, 0.09726942330598831), (3, 0.05707691237330437), (4, 0.02027188055217266), (5, 0.07541332393884659), (6, 0.02823684737086296), (7, 0.02654697746038437), (8, 0.05392530560493469), (9, 0.02374528907239437), (0, 0.03581937402486801), (1, 0.04387195035815239), (2, 0.023647697642445564), (3, 0.03569658845663071), (4, 0.026190636679530144), (5, 0.027344167232513428), (6, 0.030466478317975998), (7, 0.030725205317139626), (8, 0.022957684472203255), (9, 0.03396356850862503), (0, 0.02081502601504326), (1, 0.022325947880744934), (2, 0.03180359676480293), (3, 0.012348556891083717), (4, 0.014684025198221207), (5, 0.02519790455698967), (6, 0.017635764554142952), (7, 0.015646887943148613), (8, 0.014902740716934204), (9, 0.016878757625818253), (0, 0.01055554673075676), (1, 0.011872043833136559), (2, 0.025053955614566803), (3, 0.010068328119814396), (4, 0.017971575260162354), (5, 0.014357848092913628), (6, 0.011103099212050438), (7, 0.02468130737543106), (8, 0.008820775896310806), (9, 0.02521851845085621), (0, 0.013050161302089691), (1, 0.022147901356220245), (2, 0.007964656688272953), (3, 0.01857253536581993), (4, 0.04671047627925873), (5, 0.08727244287729263), (6, 0.11022308468818665), (7, 0.04312805086374283), (8, 0.11646446585655212), (9, 0.00796719454228878)]