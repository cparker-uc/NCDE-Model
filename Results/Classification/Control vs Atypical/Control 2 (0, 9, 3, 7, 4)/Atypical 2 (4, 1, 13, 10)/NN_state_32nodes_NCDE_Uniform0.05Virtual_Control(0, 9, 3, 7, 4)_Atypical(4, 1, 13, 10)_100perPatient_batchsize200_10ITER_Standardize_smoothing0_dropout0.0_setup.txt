Model Setup for Uniformvirtual {PATIENT_GROUPS} Trained Network:

NCDE Network Architecture Parameters
Input channels=3
Hidden channels=32
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=10
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control', 'Atypical']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=Standardize
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: (0, 9, 3, 7, 4)
MDD: (4, 1, 13, 10)
Training batch size=200
Training Results:
Runtime=220.1117582321167
Loss over time=[(0, 0.6467348337173462), (1, 0.6428205966949463), (2, 0.6094564199447632), (3, 0.6038569808006287), (4, 0.6239425539970398), (5, 0.5818279385566711), (6, 0.5719340443611145), (7, 0.55877286195755), (8, 0.5699566006660461), (9, 0.5599064826965332), (0, 0.5616463422775269), (1, 0.5163748264312744), (2, 0.524172842502594), (3, 0.5166828036308289), (4, 0.515800416469574), (5, 0.5270940065383911), (6, 0.5778558850288391), (7, 0.481806218624115), (8, 0.5072014331817627), (9, 0.5010837912559509), (0, 0.46351128816604614), (1, 0.4805089831352234), (2, 0.474741131067276), (3, 0.5403782725334167), (4, 0.4760737717151642), (5, 0.4552363157272339), (6, 0.4661877453327179), (7, 0.44527095556259155), (8, 0.4780842959880829), (9, 0.47422316670417786), (0, 0.4416714906692505), (1, 0.4369429647922516), (2, 0.4491775929927826), (3, 0.43328461050987244), (4, 0.3930467963218689), (5, 0.4593524932861328), (6, 0.40207186341285706), (7, 0.43439576029777527), (8, 0.3736327290534973), (9, 0.3844297528266907), (0, 0.46283358335494995), (1, 0.39523616433143616), (2, 0.4119187891483307), (3, 0.37019044160842896), (4, 0.3555879294872284), (5, 0.3644580841064453), (6, 0.3485599160194397), (7, 0.3349998891353607), (8, 0.36696159839630127), (9, 0.36779940128326416), (0, 0.29375725984573364), (1, 0.34602200984954834), (2, 0.3653427064418793), (3, 0.3567308783531189), (4, 0.31393560767173767), (5, 0.31047874689102173), (6, 0.2978779077529907), (7, 0.348428875207901), (8, 0.2861175835132599), (9, 0.30955636501312256), (0, 0.3447735607624054), (1, 0.31844374537467957), (2, 0.25784218311309814), (3, 0.35119178891181946), (4, 0.2703690826892853), (5, 0.24134518206119537), (6, 0.26833027601242065), (7, 0.22032789885997772), (8, 0.21471451222896576), (9, 0.22640489041805267), (0, 0.277631938457489), (1, 0.20896205306053162), (2, 0.20276623964309692), (3, 0.2267189770936966), (4, 0.19704212248325348), (5, 0.17950284481048584), (6, 0.14883096516132355), (7, 0.22975361347198486), (8, 0.12630783021450043), (9, 0.1762547641992569), (0, 0.1495383232831955), (1, 0.15943492949008942), (2, 0.1300550252199173), (3, 0.13544510304927826), (4, 0.08599113672971725), (5, 0.11080911010503769), (6, 0.1313685029745102), (7, 0.11273657530546188), (8, 0.11972260475158691), (9, 0.11024090647697449), (0, 0.20743457973003387), (1, 0.16357900202274323), (2, 0.12424767762422562), (3, 0.09371168166399002), (4, 0.10125861316919327), (5, 0.14369717240333557), (6, 0.09561396390199661), (7, 0.10789293050765991), (8, 0.11983608454465866), (9, 0.058526478707790375)]