Model Setup for Uniformvirtual {PATIENT_GROUPS} Trained Network:

NCDE Network Architecture Parameters
Input channels=3
Hidden channels=32
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=10
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control', 'Atypical']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=Standardize
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: (0, 9, 3, 7, 4)
MDD: (0, 7, 12, 8, 11)
Training batch size=200
Training Results:
Runtime=183.21237921714783
Loss over time=[(0, 0.6575223803520203), (1, 0.6537842750549316), (2, 0.6517582535743713), (3, 0.6511508226394653), (4, 0.6062435507774353), (5, 0.5876559615135193), (6, 0.5897852182388306), (7, 0.5706413388252258), (8, 0.5537288188934326), (9, 0.6367425322532654), (0, 0.6381329298019409), (1, 0.5479840040206909), (2, 0.6057855486869812), (3, 0.5692694187164307), (4, 0.5238300561904907), (5, 0.5182814598083496), (6, 0.5520788431167603), (7, 0.4736157953739166), (8, 0.5204034447669983), (9, 0.5198980569839478), (0, 0.5472974181175232), (1, 0.5258023738861084), (2, 0.5286797881126404), (3, 0.5201997756958008), (4, 0.47533875703811646), (5, 0.48625126481056213), (6, 0.4281756579875946), (7, 0.45970380306243896), (8, 0.3722478449344635), (9, 0.40191659331321716), (0, 0.4174725413322449), (1, 0.402410089969635), (2, 0.42704904079437256), (3, 0.40312328934669495), (4, 0.3720434904098511), (5, 0.3471256494522095), (6, 0.40099745988845825), (7, 0.35632550716400146), (8, 0.3071727156639099), (9, 0.34448492527008057), (0, 0.3185795545578003), (1, 0.3064619302749634), (2, 0.27151864767074585), (3, 0.27879318594932556), (4, 0.2532886266708374), (5, 0.22070784866809845), (6, 0.259608656167984), (7, 0.2083202600479126), (8, 0.193757101893425), (9, 0.20283617079257965), (0, 0.19914762675762177), (1, 0.1822405606508255), (2, 0.17793090641498566), (3, 0.15753133594989777), (4, 0.16405782103538513), (5, 0.1307971030473709), (6, 0.11279767751693726), (7, 0.24383026361465454), (8, 0.15119348466396332), (9, 0.1781661957502365), (0, 0.12196653336286545), (1, 0.13892832398414612), (2, 0.21856006979942322), (3, 0.17973962426185608), (4, 0.130279079079628), (5, 0.12268099933862686), (6, 0.1574731320142746), (7, 0.14733870327472687), (8, 0.1258978396654129), (9, 0.09638351202011108), (0, 0.098670594394207), (1, 0.08725061267614365), (2, 0.0992198958992958), (3, 0.151247039437294), (4, 0.08389005810022354), (5, 0.07792305201292038), (6, 0.10114271193742752), (7, 0.11501536518335342), (8, 0.07752452045679092), (9, 0.043322108685970306), (0, 0.13468553125858307), (1, 0.12595601379871368), (2, 0.08629400283098221), (3, 0.06986143440008163), (4, 0.11597834527492523), (5, 0.11504142731428146), (6, 0.04765528067946434), (7, 0.06825350224971771), (8, 0.062061209231615067), (9, 0.14520974457263947), (0, 0.08438032120466232), (1, 0.05365905165672302), (2, 0.0723603218793869), (3, 0.08276868611574173), (4, 0.06502819061279297), (5, 0.05907773971557617), (6, 0.06845515966415405), (7, 0.06185371056199074), (8, 0.06884575635194778), (9, 0.057026609778404236)]