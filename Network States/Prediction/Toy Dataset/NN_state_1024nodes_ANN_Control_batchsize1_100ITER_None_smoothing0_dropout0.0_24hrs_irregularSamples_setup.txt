Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=1024
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.0001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=721.950826883316
Loss over time=[(0, 13451.802693084739), (0, 13.251771522547939), (0, 14.24540708745297), (0, 15.281952089527904), (0, 16.183086709913653), (0, 16.911698608881803), (0, 17.47112969713864), (0, 17.876679367709183), (0, 18.146630389619578), (0, 18.299534807782337), (0, 18.353578153506337), (0, 18.326530612488458), (0, 18.235713131026476), (0, 18.09774630376276), (0, 17.92778825296988), (0, 17.72621262826933), (0, 17.57737050496317), (0, 17.38175522029574), (0, 17.210389885507748), (0, 17.05853832918826), (0, 16.933506456543732), (0, 16.814828188893546), (0, 16.723450319389055), (0, 16.650536365850602), (0, 16.59381161749773), (0, 16.550684938514035), (0, 16.520066465446728), (0, 16.496814259342933), (0, 16.481897283621063), (0, 16.472873804757675), (0, 16.468839655916245), (0, 16.467253325896653), (0, 16.468538828949487), (0, 16.471745698910677), (0, 16.476529152087213), (0, 16.48140795749063), (0, 16.487191353634252), (0, 16.493203403109977), (0, 16.499144714610736), (0, 16.50513048953977), (0, 16.511368416790756), (0, 16.516327080085535), (0, 16.52150098179254), (0, 16.526472087735826), (0, 16.531314808739737), (0, 16.53497728824407), (0, 16.538739611692744), (0, 16.54218212814758), (0, 16.545448497748342), (0, 16.551217665519065), (0, 16.550657133854518), (0, 16.55299538058532), (0, 16.555056045104056), (0, 16.556857080059057), (0, 16.558417525228723), (0, 16.55975601421125), (0, 16.560890456240994), (0, 16.561837892553974), (0, 16.56261441782286), (0, 16.563235137479257), (0, 16.56371416855603), (0, 16.56406464042266), (0, 16.56429872412365), (0, 16.56442766261115), (0, 16.564461815610755), (0, 16.56441069807303), (0, 16.564283035902584), (0, 16.56408680720923), (0, 16.563829294585293), (0, 16.5635171354257), (0, 16.56315635975547), (0, 16.562752448114924), (0, 16.562310362527686), (0, 16.56183459472444), (0, 16.561329202274063), (0, 16.560797845903057), (0, 16.56024382201652), (0, 16.55967009354322), (0, 16.55907932185012), (0, 16.55847389285705), (0, 16.557855941664574), (0, 16.55722737368337), (0, 16.55658989169528), (0, 16.5559450101436), (0, 16.555294073421795), (0, 16.55463827791375), (0, 16.55397868089525), (0, 16.553316214500647), (0, 16.552651701231937), (0, 16.551985862807086), (0, 16.551319334764006), (0, 16.5506526708236), (0, 16.54998635156233), (0, 16.5493207959224), (0, 16.548656366689798), (0, 16.547993374404026), (0, 16.547332085261726), (0, 16.546672724416116), (0, 16.54601548523247), (0, 16.545360526511804)]