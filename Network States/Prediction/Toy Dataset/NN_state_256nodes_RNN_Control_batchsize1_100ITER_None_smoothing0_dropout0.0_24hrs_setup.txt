Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=256
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.0003
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=1
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=42.392438650131226
Loss over time=[(0, 31.894119262695312), (0, 31.316017150878906), (0, 30.34444236755371), (0, 28.784643173217773), (0, 27.130210876464844), (0, 26.064422607421875), (0, 25.263051986694336), (0, 24.38018226623535), (0, 23.49773406982422), (0, 22.84271240234375), (0, 22.20819664001465), (0, 21.676239013671875), (0, 21.280048370361328), (0, 20.94730567932129), (0, 20.57932472229004), (0, 20.167564392089844), (0, 19.66889762878418), (0, 19.215965270996094), (0, 18.974260330200195), (0, 18.744169235229492), (0, 18.492860794067383), (0, 18.26059341430664), (0, 18.084596633911133), (0, 17.84424591064453), (0, 17.570045471191406), (0, 17.288707733154297), (0, 16.983797073364258), (0, 16.621734619140625), (0, 16.35896873474121), (0, 16.241403579711914), (0, 16.15020179748535), (0, 16.051149368286133), (0, 15.970633506774902), (0, 15.888567924499512), (0, 15.796699523925781), (0, 15.701367378234863), (0, 15.606419563293457), (0, 15.506840705871582), (0, 15.367648124694824), (0, 15.096430778503418), (0, 14.886767387390137), (0, 14.773174285888672), (0, 14.689263343811035), (0, 14.615787506103516), (0, 14.54544448852539), (0, 14.480488777160645), (0, 14.420844078063965), (0, 14.362849235534668), (0, 14.309410095214844), (0, 14.263867378234863), (0, 14.225635528564453), (0, 14.189289093017578), (0, 14.16148853302002), (0, 14.128372192382812), (0, 14.08916187286377), (0, 14.049906730651855), (0, 14.017688751220703), (0, 13.98498821258545), (0, 13.950423240661621), (0, 13.916958808898926), (0, 13.88628101348877), (0, 13.85628890991211), (0, 13.823097229003906), (0, 13.788875579833984), (0, 13.756381034851074), (0, 13.725869178771973), (0, 13.697064399719238), (0, 13.670125007629395), (0, 13.64471435546875), (0, 13.6201171875), (0, 13.59646224975586), (0, 13.57343578338623), (0, 13.549818992614746), (0, 13.52647876739502), (0, 13.504180908203125), (0, 13.482436180114746), (0, 13.46078872680664), (0, 13.439362525939941), (0, 13.41838550567627), (0, 13.397811889648438), (0, 13.377537727355957), (0, 13.357413291931152), (0, 13.337417602539062), (0, 13.317584991455078), (0, 13.297779083251953), (0, 13.278038024902344), (0, 13.258605003356934), (0, 13.239394187927246), (0, 13.220821380615234), (0, 13.202743530273438), (0, 13.184856414794922), (0, 13.167574882507324), (0, 13.150497436523438), (0, 13.133323669433594), (0, 13.115405082702637), (0, 13.0960111618042), (0, 13.074795722961426), (0, 13.079676628112793), (0, 13.220669746398926), (0, 13.180500984191895)]