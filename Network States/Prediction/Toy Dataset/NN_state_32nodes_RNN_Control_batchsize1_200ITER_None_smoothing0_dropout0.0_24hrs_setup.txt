Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=32
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.0001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=1
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=423.2415180206299
Loss over time=[(0, 32.928306579589844), (0, 32.02312088012695), (0, 31.83551025390625), (0, 31.72791862487793), (0, 31.64552116394043), (0, 31.574560165405273), (0, 31.509994506835938), (0, 31.449556350708008), (0, 31.392107009887695), (0, 31.33684539794922), (0, 31.283416748046875), (0, 31.23150634765625), (0, 31.180944442749023), (0, 31.1314697265625), (0, 31.08254051208496), (0, 31.03369903564453), (0, 30.98493766784668), (0, 30.936445236206055), (0, 30.88832664489746), (0, 30.840553283691406), (0, 30.793045043945312), (0, 30.745697021484375), (0, 30.69843292236328), (0, 30.651182174682617), (0, 30.60392189025879), (0, 30.556617736816406), (0, 30.50911521911621), (0, 30.461524963378906), (0, 30.41377067565918), (0, 30.365827560424805), (0, 30.31768226623535), (0, 30.26933479309082), (0, 30.220815658569336), (0, 30.172210693359375), (0, 30.124114990234375), (0, 30.11509895324707), (0, 30.030370712280273), (0, 29.98267364501953), (0, 29.93564796447754), (0, 29.88840675354004), (0, 29.840805053710938), (0, 29.79286003112793), (0, 29.74454689025879), (0, 29.695884704589844), (0, 29.647226333618164), (0, 29.598281860351562), (0, 29.549428939819336), (0, 29.501523971557617), (0, 29.45224952697754), (0, 29.408523559570312), (0, 29.357149124145508), (0, 29.31005096435547), (0, 29.262609481811523), (0, 29.21484375), (0, 29.166845321655273), (0, 29.118566513061523), (0, 29.06988525390625), (0, 29.021753311157227), (0, 28.973724365234375), (0, 28.92503547668457), (0, 28.875890731811523), (0, 28.826322555541992), (0, 28.77642250061035), (0, 28.7262020111084), (0, 28.67561149597168), (0, 28.624717712402344), (0, 28.573556900024414), (0, 28.52208137512207), (0, 28.47038459777832), (0, 28.418420791625977), (0, 28.3663330078125), (0, 28.314302444458008), (0, 28.263959884643555), (0, 28.208654403686523), (0, 28.156204223632812), (0, 28.10204315185547), (0, 28.04767608642578), (0, 27.993837356567383), (0, 27.9392147064209), (0, 27.8844051361084), (0, 27.829256057739258), (0, 27.77376365661621), (0, 27.717920303344727), (0, 27.66175079345703), (0, 27.605262756347656), (0, 27.548479080200195), (0, 27.491422653198242), (0, 27.43410301208496), (0, 27.376550674438477), (0, 27.318817138671875), (0, 27.26108741760254), (0, 27.202743530273438), (0, 27.1445255279541), (0, 27.08608055114746), (0, 27.027456283569336), (0, 26.96877098083496), (0, 26.910303115844727), (0, 26.855257034301758), (0, 26.79395866394043), (0, 26.735109329223633), (0, 26.676599502563477), (0, 26.61821937561035), (0, 26.5599422454834), (0, 26.501787185668945), (0, 26.44380760192871), (0, 26.386030197143555), (0, 26.328493118286133), (0, 26.2712345123291), (0, 26.214311599731445), (0, 26.170412063598633), (0, 26.10231590270996), (0, 26.046960830688477), (0, 25.992050170898438), (0, 25.937536239624023), (0, 25.883583068847656), (0, 25.829910278320312), (0, 25.776845932006836), (0, 25.724035263061523), (0, 25.672197341918945), (0, 25.623796463012695), (0, 25.569442749023438), (0, 25.518447875976562), (0, 25.46840476989746), (0, 25.41883659362793), (0, 25.369674682617188), (0, 25.32094383239746), (0, 25.272668838500977), (0, 25.224870681762695), (0, 25.177560806274414), (0, 25.130739212036133), (0, 25.08440399169922), (0, 25.038551330566406), (0, 24.993179321289062), (0, 24.948280334472656), (0, 24.90385627746582), (0, 24.85990333557129), (0, 24.81641387939453), (0, 24.77338981628418), (0, 24.730819702148438), (0, 24.68869972229004), (0, 24.64704132080078), (0, 25.070465087890625), (0, 24.57916259765625), (0, 24.552352905273438), (0, 24.52057456970215), (0, 24.49113655090332), (0, 24.461862564086914), (0, 24.432714462280273), (0, 24.40369415283203), (0, 24.37481117248535), (0, 24.346086502075195), (0, 24.31751251220703), (0, 24.28910255432129), (0, 24.26085090637207), (0, 24.232744216918945), (0, 24.20477867126465), (0, 24.17696189880371), (0, 24.14931297302246), (0, 24.121828079223633), (0, 24.094512939453125), (0, 24.067359924316406), (0, 24.040369033813477), (0, 24.013547897338867), (0, 23.98688507080078), (0, 23.960390090942383), (0, 23.93406105041504), (0, 23.90789794921875), (0, 23.881895065307617), (0, 23.856063842773438), (0, 23.830392837524414), (0, 23.804893493652344), (0, 23.779558181762695), (0, 23.754392623901367), (0, 23.729385375976562), (0, 23.704544067382812), (0, 23.67986297607422), (0, 23.655344009399414), (0, 23.630971908569336), (0, 23.60675811767578), (0, 23.58268928527832), (0, 23.559249877929688), (0, 23.544647216796875), (0, 23.51494789123535), (0, 23.49066162109375), (0, 23.46714973449707), (0, 23.44391441345215), (0, 23.420827865600586), (0, 23.397842407226562), (0, 23.374937057495117), (0, 23.352088928222656), (0, 23.329299926757812), (0, 23.306562423706055), (0, 23.28386878967285), (0, 23.26121711730957), (0, 23.238609313964844), (0, 23.21604347229004), (0, 23.19352149963379), (0, 23.171045303344727), (0, 23.14861488342285), (0, 23.12623405456543)]