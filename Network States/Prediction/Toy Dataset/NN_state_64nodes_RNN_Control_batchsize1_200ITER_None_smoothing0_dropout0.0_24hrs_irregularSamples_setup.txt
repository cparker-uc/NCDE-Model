Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=64
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=1932.172401189804
Loss over time=[(0, 15.070158341766183), (0, 13.685905545793894), (0, 12.778276688341686), (0, 11.092592107289589), (0, 7.613761490795238), (0, 3.5589587880343525), (0, 1.6415700958041757), (0, 1.2637904771850952), (0, 1.210556596687776), (0, 1.182563323604079), (0, 1.141924457579177), (0, 1.086016474650061), (0, 1.0164194098694435), (0, 0.9672456012227317), (0, 0.884999157143188), (0, 0.8325324183398588), (0, 0.777139227941486), (0, 0.7206999181662339), (0, 0.6648917711934694), (0, 0.6106074132608691), (0, 0.5623742123821477), (0, 0.5261857441966865), (0, 0.5091421531936136), (0, 0.5162412996828809), (0, 0.5302192479481792), (0, 0.5200536317679284), (0, 0.48929985745139143), (0, 0.5224839879085804), (0, 0.45992136200998096), (0, 0.4652017849964474), (0, 0.470430123606441), (0, 0.47355409442485735), (0, 0.47425463284777053), (0, 0.47265344648232727), (0, 0.4691177836093969), (0, 0.4640992048390507), (0, 0.45802607524729716), (0, 0.45126162847195633), (0, 0.4441038687351659), (0, 0.4367913040341816), (0, 0.42949830858087185), (0, 0.4223328846132062), (0, 0.4153572759021795), (0, 0.40863274288491364), (0, 0.40227088986994586), (0, 0.3964907085543725), (0, 0.39173837583955234), (0, 0.3889949430700748), (0, 0.3902274138648946), (0, 0.3963956906870591), (0, 0.39821721051800524), (0, 0.39292497803527787), (0, 0.38976908545140915), (0, 0.38929175202750216), (0, 0.38989562851819426), (0, 0.3906304744388356), (0, 0.39120311995227547), (0, 0.39147618033980747), (0, 0.3914267522242538), (0, 0.39108212828044225), (0, 0.39048776274246816), (0, 0.38965580651955345), (0, 0.38859728845496466), (0, 0.3874130143381144), (0, 0.38641577551057615), (0, 0.38596935991162035), (0, 0.3852939046508443), (0, 0.38384547074337627), (0, 0.3811625389747005), (0, 0.3719281738433078), (0, 0.3540401080278813), (0, 0.3479610974734518), (0, 0.3685915712848946), (0, 0.33799614902854297), (0, 0.3671041024021053), (0, 0.3506359836554766), (0, 0.35228103054323495), (0, 0.3334223613837816), (0, 0.3466281755802146), (0, 0.35511723150972957), (0, 0.3663396947150417), (0, 0.5097081802967337), (0, 0.4112660859407578), (0, 0.45216531212325056), (0, 0.49790538158900893), (0, 0.49578757779814775), (0, 0.5214567663063191), (0, 0.4776457004733947), (0, 0.5028955262285224), (0, 0.5097714876253924), (0, 0.5544887423433728), (0, 0.5528092894066801), (0, 0.5237099058042566), (0, 0.4850552899391503), (0, 0.4617386719602856), (0, 0.4600891909750253), (0, 0.45926495082715446), (0, 0.4547711290358007), (0, 0.4518632875572157), (0, 0.4502059660515004), (0, 0.4461746688994394), (0, 0.4343507155744186), (0, 0.4607185878260287), (0, 0.43336600904727285), (0, 0.42580483287579823), (0, 0.4226984878784838), (0, 0.42004907218528187), (0, 0.41711267642985167), (0, 0.41372421843942137), (0, 0.40989099239552546), (0, 0.4056786967603072), (0, 0.4045547027945721), (0, 0.4026598711078377), (0, 0.40010916790723566), (0, 0.39705053936541757), (0, 0.3936703398936771), (0, 0.39028218348426186), (0, 0.3871558921226934), (0, 0.3846143910250839), (0, 0.38276944206364644), (0, 0.38142850026731584), (0, 0.380373919866044), (0, 0.37933433080049483), (0, 0.37823993064803113), (0, 0.37703887061348723), (0, 0.37573414240133496), (0, 0.3743538135096382), (0, 0.3728761166728063), (0, 0.37132037519434186), (0, 0.3696845739866757), (0, 0.3679702710581066), (0, 0.3661874285561309), (0, 0.3643596805161877), (0, 0.3625130581794282), (0, 0.36072490414252273), (0, 0.35893726150844224), (0, 0.357281984274286), (0, 0.3556055869600445), (0, 0.35397804807299765), (0, 0.3523594012929641), (0, 0.3508230659261224), (0, 0.34929152277953573), (0, 0.34768573937306707), (0, 0.3460778062961465), (0, 0.34550768430045276), (0, 0.3577956150727533), (0, 0.357896344350285), (0, 0.3642755851794768), (0, 0.3754495455687798), (0, 0.3861202843416886), (0, 0.40030649173809146), (0, 0.40554242675459523), (0, 0.45688713777253814), (0, 0.6318257832591355), (0, 0.47287093297573296), (0, 0.6923399223790372), (0, 0.722752120959265), (0, 0.6365754726406337), (0, 0.7286081519271067), (0, 0.6111012700445403), (0, 0.6224639448455457), (0, 0.663415674797225), (0, 0.6405483366512057), (0, 0.5999900890169877), (0, 0.5186782257686738), (0, 0.5450747803333885), (0, 0.5542063575133879), (0, 0.5371722421739186), (0, 0.5107914300574342), (0, 0.48780894789183193), (0, 0.46211159507366345), (0, 0.43713279654957665), (0, 0.4284791042971583), (0, 0.42853191433690085), (0, 0.4247132411240219), (0, 0.45588633517661054), (0, 0.47597004395264664), (0, 0.4939444502661321), (0, 0.5643395081316154), (0, 0.5735465338394036), (0, 0.6200936120231865), (0, 0.6335468125338541), (0, 0.7410258351282145), (0, 0.6769700098138222), (0, 0.7580485520736375), (0, 0.6622884522238497), (0, 0.7087971890343661), (0, 0.7337094464706189), (0, 0.8108027044056803), (0, 0.8700819502822035), (0, 0.8082425872025073), (0, 0.6866601054994707), (0, 0.6399457629921134), (0, 0.7041115799696329), (0, 0.7730950866703837), (0, 0.834054563702114), (0, 0.8567089865306894), (0, 0.8607036426485345), (0, 0.8505926021998319), (0, 0.8197607757322154)]