Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=16
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=639.017245054245
Loss over time=[(0, 14.140152023100958), (0, 13.868793085615728), (0, 13.655929564658196), (0, 13.480624590681332), (0, 13.31775805871848), (0, 13.202990300156648), (0, 13.087240841418264), (0, 12.977838886712945), (0, 12.869739537183458), (0, 12.758226446057131), (0, 12.63856190317908), (0, 12.505778586438725), (0, 12.354609222227763), (0, 12.179308054942716), (0, 11.973612854757421), (0, 11.731323705145298), (0, 11.446455501211323), (0, 11.113636591735421), (0, 10.728552249100808), (0, 10.288942092521992), (0, 9.79605564640985), (0, 9.255186366774899), (0, 8.675497381024543), (0, 8.069016986311757), (0, 7.448758610718883), (0, 6.827225618553513), (0, 6.215292550450489), (0, 5.625290538590877), (0, 5.067127425048433), (0, 4.549337877756558), (0, 4.077497980272039), (0, 3.653581287280185), (0, 3.27792049577255), (0, 2.9497952626703756), (0, 2.6662618372205924), (0, 2.4230636888310593), (0, 2.215504392444125), (0, 2.0388911105060235), (0, 1.888784841325762), (0, 1.761142065816417), (0, 1.6523812110517964), (0, 1.5593973489215311), (0, 1.479543722724942), (0, 1.4105938888462457), (0, 1.3506941352295676), (0, 1.29831673063417), (0, 1.2522526278787616), (0, 1.2112509343112354), (0, 1.174114480765291), (0, 1.1408388562789187), (0, 1.1105700673374577), (0, 1.0827441193447778), (0, 1.0569352026275771), (0, 1.032816957085599), (0, 1.0101417054062356), (0, 0.9887283276379795), (0, 0.9684580535574004), (0, 0.9492730329082425), (0, 0.931011519487527), (0, 0.9133731125786002), (0, 0.8964971666678148), (0, 0.8803417611853912), (0, 0.8648218958012999), (0, 0.8498755364697832), (0, 0.8354676499694635), (0, 0.8215835615518102), (0, 0.8082206686371457), (0, 0.7953769245454421), (0, 0.7830338701118938), (0, 0.7711497204431802), (0, 0.7596956468285909), (0, 0.748664006425268), (0, 0.7380284280001305), (0, 0.7277465781725794), (0, 0.7177772695847696), (0, 0.7080891713814981), (0, 0.6986634120279863), (0, 0.6894920715891302), (0, 0.6805738340829409), (0, 0.6719117998469027), (0, 0.6635308268406593), (0, 0.655414867081656), (0, 0.647536682425456), (0, 0.639996693371247), (0, 0.632705921828911), (0, 0.6256203517206363), (0, 0.6187388337828899), (0, 0.6121101348438804), (0, 0.6057744666137967), (0, 0.5995844961705784), (0, 0.5936519771779404), (0, 0.5879276671396018), (0, 0.5823871352742388), (0, 0.5770238683088008), (0, 0.5718323294152757), (0, 0.5668058890704958), (0, 0.5619453834140598), (0, 0.5572567239409598), (0, 0.5527396353134375), (0, 0.5483789186839294)]