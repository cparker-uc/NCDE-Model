Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=2048
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 5e-05
    maximize: False
    weight_decay: 1e-06
)Training Iterations=200
Learning rate=5e-05
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=716.5599117279053
Loss over time=[(0, 25.30270265304611), (0, 23.18055412665413), (0, 11.947176397897541), (0, 11.423597023928256), (0, 11.382258024394538), (0, 11.34556245326271), (0, 11.324825060534915), (0, 11.28591501796063), (0, 11.171401532911313), (0, 10.948570121574665), (0, 10.67756491712697), (0, 10.530054827116894), (0, 9.560223872360666), (0, 8.735517145053976), (0, 9.452822015427444), (0, 8.506493410942054), (0, 9.287247539151144), (0, 10.273494877238418), (0, 11.342623641527032), (0, 12.389486431315051), (0, 13.342808764624937), (0, 14.15971532097811), (0, 14.820656175476556), (0, 15.32739574124362), (0, 15.708211817055483), (0, 16.042227223777946), (0, 16.51922095648454), (0, 17.23174426710925), (0, 17.21954407202594), (0, 17.004441900038817), (0, 16.995535389435386), (0, 17.018335035471566), (0, 16.958701478524013), (0, 16.775830501782483), (0, 16.46529250929702), (0, 16.043128692648263), (0, 15.541213826843663), (0, 15.008150535716782), (0, 14.507786232099178), (0, 14.07938340512377), (0, 13.622744264094159), (0, 13.016900589298691), (0, 12.38942619882472), (0, 11.826570484937598), (0, 11.302904257989072), (0, 10.783923961858502), (0, 10.257368232102166), (0, 9.690232394124976), (0, 9.498180943912345), (0, 9.317805434800148), (0, 9.168023274881183), (0, 9.044388307532774), (0, 8.940349340565719), (0, 8.843551247122486), (0, 8.737659504180959), (0, 8.611628396195853), (0, 8.466169802084716), (0, 8.308372923158537), (0, 8.143171633032544), (0, 7.971177340414393), (0, 7.790971265699994), (0, 7.601467318396184), (0, 7.40296152165745), (0, 7.197202004119699), (0, 6.986993693981041), (0, 6.775642788415573), (0, 6.566383419368837), (0, 6.3618864022222095), (0, 6.163958368382252), (0, 5.973543459553753), (0, 5.791050207348456), (0, 5.616843246743146), (0, 5.4515990939185475), (0, 5.296325932306135), (0, 5.152133772235206), (0, 5.020055160227983), (0, 4.901297218358556), (0, 4.79906433832579), (0, 4.738433218157638), (0, 568.4183872232029), (0, 5.237366166864464), (0, 7.282615842236016), (0, 10.081095299661335), (0, 13.150976577547997), (0, 16.18431830591266), (0, 19.18900911018844), (0, 22.32758681913989), (0, 25.716401035043976), (0, 29.218473850339645), (0, 32.50909933488869), (0, 35.314061067687106), (0, 37.51555672355861), (0, 39.11554744778842), (0, 40.17030595780738), (0, 40.74996757076991), (0, 40.925081800309655), (0, 40.76848475752941), (0, 40.36194561064085), (0, 39.799635045037306), (0, 39.18274684535296), (0, 38.604403825785546), (0, 38.13124190513574), (0, 37.792680204773696), (0, 37.58428521213505), (0, 37.4813038516777), (0, 37.45306262235494), (0, 37.47212670171961), (0, 37.517723711281334), (0, 37.57567175913496), (0, 37.63697334994943), (0, 37.696268480028145), (0, 37.75058607898756), (0, 37.79845937527159), (0, 37.83933770233894), (0, 37.873211571928294), (0, 37.900377437363915), (0, 37.921292956119835), (0, 37.93648955740218), (0, 37.94651904949769), (0, 37.951924099580076), (0, 37.95322000334319), (0, 37.95088690857277), (0, 37.94536556890543), (0, 37.937056902316975), (0, 37.926322869305345), (0, 37.913488909729054), (0, 37.898845970393786), (0, 37.88265388613582), (0, 37.86514340192259), (0, 37.84651980308371), (0, 37.826964448670594), (0, 37.8066377973301), (0, 37.78568107596963), (0, 37.76421865699091), (0, 37.74235954409155), (0, 37.72019928038733), (0, 37.69782108664771), (0, 37.67529754683457), (0, 37.652691218361056), (0, 37.63005648063594), (0, 37.60744000257254), (0, 37.58488157155055), (0, 37.56241538967483), (0, 37.540069905498044), (0, 37.51786925758194), (0, 37.49583340706174), (0, 37.47397903780447), (0, 37.452319689771585), (0, 37.430866069011515), (0, 37.40962705326254), (0, 37.38860954913866), (0, 37.367818774199925), (0, 37.34725870712097), (0, 37.326932672961206), (0, 37.30684280853077), (0, 37.2869911914078), (0, 37.26737926473341), (0, 37.24800869694672), (0, 37.228881046832264), (0, 37.20999819900984), (0, 37.191362391718606), (0, 37.172976503104856), (0, 37.154843778305214), (0, 37.136968396572236), (0, 37.11935539678794), (0, 37.10201059048763), (0, 37.08494079908908), (0, 37.06815383818896), (0, 37.051658335857965), (0, 37.03546396918002), (0, 37.0195813008766), (0, 37.00402159669527), (0, 36.98879706009557), (0, 36.973920106986924), (0, 36.95940386492187), (0, 36.94526145679893), (0, 36.93150571477373), (0, 36.91814952180294), (0, 36.905204955438286), (0, 36.8926830642792), (0, 36.88059369370736), (0, 36.868944998895785), (0, 36.85774304505978), (0, 36.84699168150154), (0, 36.83669204657103), (0, 36.82684255516831), (0, 36.81743835617176), (0, 36.80847173406184), (0, 36.79993168490644), (0, 36.791804004963694), (0, 36.78407149227929), (0, 36.77671419904884), (0, 36.7697097594189), (0, 36.763033762808774), (0, 36.7566601209905), (0, 36.750562162449285), (0, 36.74471258435197), (0, 36.73908426218649), (0, 36.73365078425635), (0, 36.72838694961837)]