Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=64
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.0001
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=1
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=29.436870098114014
Loss over time=[(0, 31.924829483032227), (0, 31.86441993713379), (0, 31.806955337524414), (0, 31.752090454101562), (0, 31.699310302734375), (0, 31.648117065429688), (0, 31.59814453125), (0, 31.549026489257812), (0, 31.500320434570312), (0, 31.450899124145508), (0, 31.39928436279297), (0, 31.343225479125977), (0, 31.285268783569336), (0, 31.22581672668457), (0, 31.164466857910156), (0, 31.10150146484375), (0, 31.03672981262207), (0, 30.969858169555664), (0, 30.898767471313477), (0, 30.82404136657715), (0, 30.745223999023438), (0, 30.661596298217773), (0, 30.572296142578125), (0, 30.476354598999023), (0, 30.371915817260742), (0, 30.25745391845703), (0, 30.133180618286133), (0, 29.999086380004883), (0, 29.85540008544922), (0, 29.702659606933594), (0, 29.541725158691406), (0, 29.37378692626953), (0, 29.200376510620117), (0, 29.022722244262695), (0, 28.84307861328125), (0, 28.66283416748047), (0, 28.485023498535156), (0, 28.31184196472168), (0, 28.14496612548828), (0, 27.98541259765625), (0, 27.833389282226562), (0, 27.688369750976562), (0, 27.549209594726562), (0, 27.414403915405273), (0, 27.282434463500977), (0, 27.15199851989746), (0, 27.022188186645508), (0, 26.89251708984375), (0, 26.762842178344727), (0, 26.631616592407227), (0, 26.500272750854492), (0, 26.369665145874023), (0, 26.240646362304688), (0, 26.114015579223633), (0, 25.98841094970703), (0, 25.865516662597656), (0, 25.745956420898438), (0, 25.629961013793945), (0, 25.51744842529297), (0, 25.408050537109375), (0, 25.301172256469727), (0, 25.196043014526367), (0, 25.091772079467773), (0, 24.987449645996094), (0, 24.882253646850586), (0, 24.775476455688477), (0, 24.666555404663086), (0, 24.555994033813477), (0, 24.444486618041992), (0, 24.332902908325195), (0, 24.222116470336914), (0, 24.11273765563965), (0, 24.00538444519043), (0, 23.900625228881836), (0, 23.79880714416504), (0, 23.700143814086914), (0, 23.60474395751953), (0, 23.51262855529785), (0, 23.42372703552246), (0, 23.337905883789062), (0, 23.254974365234375), (0, 23.174734115600586), (0, 23.09694480895996), (0, 23.021345138549805), (0, 22.947587966918945), (0, 22.875259399414062), (0, 22.803842544555664), (0, 22.732770919799805), (0, 22.66145896911621), (0, 22.589387893676758), (0, 22.516157150268555), (0, 22.44157600402832), (0, 22.365707397460938), (0, 22.288909912109375), (0, 22.21184730529785), (0, 22.13541603088379), (0, 22.060598373413086), (0, 21.988317489624023), (0, 21.91927146911621), (0, 21.85383415222168), (0, 21.79204559326172), (0, 21.733673095703125), (0, 21.67828941345215), (0, 21.6253662109375), (0, 21.57436180114746), (0, 21.52473258972168), (0, 21.475976943969727), (0, 21.427640914916992), (0, 21.379289627075195), (0, 21.330537796020508), (0, 21.281034469604492), (0, 21.230478286743164), (0, 21.178647994995117), (0, 21.125396728515625), (0, 21.070701599121094), (0, 21.014673233032227), (0, 20.957563400268555), (0, 20.899751663208008), (0, 20.841753005981445), (0, 20.784135818481445), (0, 20.727195739746094), (0, 20.669416427612305), (0, 20.612234115600586), (0, 20.55634880065918), (0, 20.50214958190918), (0, 20.449810028076172), (0, 20.39935874938965), (0, 20.350717544555664), (0, 20.30375099182129), (0, 20.258291244506836), (0, 20.21416473388672), (0, 20.17120933532715), (0, 20.1292724609375), (0, 20.088220596313477), (0, 20.047941207885742), (0, 20.008333206176758), (0, 19.96931266784668), (0, 19.930816650390625), (0, 19.892784118652344), (0, 19.855165481567383), (0, 19.817916870117188), (0, 19.781003952026367), (0, 19.74439811706543), (0, 19.70807456970215), (0, 19.67193031311035), (0, 19.633821487426758), (0, 19.595727920532227), (0, 19.558486938476562), (0, 19.52256965637207), (0, 19.487972259521484), (0, 19.45417022705078), (0, 19.420400619506836), (0, 19.38616371154785), (0, 19.35142707824707), (0, 19.316431045532227), (0, 19.28150749206543), (0, 19.246917724609375), (0, 19.21280288696289), (0, 19.17914390563965), (0, 19.14579963684082), (0, 19.112577438354492), (0, 19.07931137084961), (0, 19.045944213867188), (0, 19.012514114379883), (0, 18.97910499572754), (0, 18.945837020874023), (0, 18.91278839111328), (0, 18.879995346069336), (0, 18.84743309020996), (0, 18.81504249572754), (0, 18.782766342163086), (0, 18.750558853149414), (0, 18.71842384338379), (0, 18.686403274536133), (0, 18.654542922973633), (0, 18.622894287109375), (0, 18.591489791870117), (0, 18.560338973999023), (0, 18.52942657470703), (0, 18.498741149902344), (0, 18.468263626098633), (0, 18.437997817993164), (0, 18.40795135498047), (0, 18.378149032592773), (0, 18.34861183166504), (0, 18.319355010986328), (0, 18.290376663208008), (0, 18.26167106628418), (0, 18.233217239379883), (0, 18.20501136779785), (0, 18.177038192749023), (0, 18.149295806884766), (0, 18.121789932250977), (0, 18.09452247619629), (0, 18.06749153137207), (0, 18.040693283081055), (0, 18.01412010192871), (0, 17.987764358520508), (0, 17.961618423461914), (0, 17.935680389404297)]