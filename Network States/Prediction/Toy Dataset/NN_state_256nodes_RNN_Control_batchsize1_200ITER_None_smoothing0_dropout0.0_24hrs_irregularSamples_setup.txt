Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=256
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.0001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=357.65814089775085
Loss over time=[(0, 14.869122028941447), (0, 14.149723107759678), (0, 13.871122557171859), (0, 13.65109724019286), (0, 13.4575983757705), (0, 13.283565483589827), (0, 13.126027899324429), (0, 12.982605501954993), (0, 12.850514906007634), (0, 12.725846397716), (0, 12.602302781830176), (0, 12.468365106550449), (0, 12.300862782214267), (0, 12.051731912332423), (0, 11.62753258952606), (0, 10.884367086275981), (0, 9.704061439964319), (0, 8.159640825267275), (0, 6.570923265700288), (0, 5.278397765938387), (0, 4.387932168364514), (0, 3.810674643803949), (0, 3.427324630300392), (0, 3.1565211772393433), (0, 2.9523369466315788), (0, 2.789671752267497), (0, 2.6543588023535314), (0, 2.5379552868199764), (0, 2.4351487501284095), (0, 2.3424670510508903), (0, 2.257694156604691), (0, 2.1803932958496763), (0, 2.2562883344475755), (0, 2.1031665091413037), (0, 2.1035848663703773), (0, 2.1041268532671595), (0, 2.1045243502270776), (0, 2.1046570451572504), (0, 2.1044534343696935), (0, 2.1038700362959943), (0, 2.1028826209618834), (0, 2.1014809641797445), (0, 2.099665179789616), (0, 2.0974429853329353), (0, 2.094827603364954), (0, 2.0918361363924953), (0, 2.088488303191339), (0, 2.0848054573021773), (0, 2.0808098308277683), (0, 2.0765239534489237), (0, 2.071970214138846), (0, 2.0671705339751685), (0, 2.0621461263902456), (0, 2.056917330400799), (0, 2.0515034977100224), (0, 2.045922922571974), (0, 2.0401928060571506), (0, 2.034329245845596), (0, 2.0283472470619723), (0, 2.022260746104781), (0, 2.016082645869821), (0, 2.009824858482237), (0, 2.003498353953605), (0, 1.9971132108484158), (0, 1.99067866846849), (0, 1.98420318094903), (0, 1.9776944681948987), (0, 1.9711595689201085), (0, 1.9646048891082648), (0, 1.9580362495370123), (0, 1.9514589312516286), (0, 1.9448777187440942), (0, 1.9382969394953318), (0, 1.9317205021808739), (0, 1.9251519318440942), (0, 1.9185944027244737), (0, 1.9120507688523203), (0, 1.9055235913119715), (0, 1.8990151659716241), (0, 1.8925275456292996), (0, 1.886062563540792), (0, 1.8796218523041468), (0, 1.8732068637623698), (0, 1.866818884882984), (0, 1.860459054124891), (0, 1.8541283748183097), (0, 1.8478277283871336), (0, 1.841557886682318), (0, 1.835319522060109), (0, 1.8291132166454256), (0, 1.8229394724642072), (0, 1.8167987184172734), (0, 1.8106913179593243), (0, 1.8046175752990292), (0, 1.798577742048921), (0, 1.7925720216611338), (0, 1.7866005753203262), (0, 1.7806635252295049), (0, 1.774760959565975), (0, 1.7688929348698288), (0, 1.7630594804182074), (0, 1.7572606001481814), (0, 1.7514962762304818), (0, 1.7457664697173028), (0, 1.7400711242052607), (0, 1.7344101670383614), (0, 1.7287835101893332), (0, 1.7231910533412584), (0, 1.7176326830751771), (0, 1.7121082757805854), (0, 1.7066176975263538), (0, 1.7011608054068095), (0, 1.6957374477061924), (0, 1.6903474653160941), (0, 1.6849906912688404), (0, 1.679666951956931), (0, 1.674376067152762), (0, 1.6691178501262443), (0, 1.663892108140379), (0, 1.6586986423126806), (0, 1.6535372478898036), (0, 1.6484077141415536), (0, 1.6433098243057103), (0, 1.6382433556752218), (0, 1.633208079355983), (0, 1.628203760008435), (0, 1.6232301560801954), (0, 1.6182870189687115), (0, 1.6133740934049907), (0, 1.6084911169230454), (0, 1.6036378195411427), (0, 1.5988139233736038), (0, 1.5940191428833652), (0, 1.589253184338614), (0, 1.5845157450258622), (0, 1.5798065140642068), (0, 1.5751251716364567), (0, 1.570471389102736), (0, 1.5658448288139273), (0, 1.5612451448693865), (0, 1.5566719824179647), (0, 1.5521249796247412), (0, 1.5476037671197735), (0, 1.5431079702789947), (0, 1.5386372104374704), (0, 1.5341911072412968), (0, 1.529769281764479), (0, 1.525371360633048), (0, 1.5209969806460732), (0, 1.5166457965230375), (0, 1.5123174886916202), (0, 1.508011774670307), (0, 1.5037284234853505), (0, 1.4994672733212064), (0, 1.495228255054025), (0, 1.4910114210209462), (0, 1.4868169823499666), (0, 1.4826453556719), (0, 1.4784972221284907), (0, 1.4743736018758895), (0, 1.470275946179279), (0, 1.466206252954355), (0, 1.4621672064888565), (0, 1.4581623468138416), (0, 1.4541962674345175), (0, 1.4502748362432385), (0, 1.446405426031668), (0, 1.4425971197542085), (0, 1.4388608275223207), (0, 1.4352092042325122), (0, 1.4316561912101484), (0, 1.4282159310871911), (0, 1.424900757283208), (0, 1.421718033684216), (0, 1.4186659636248955), (0, 1.4157292847240412), (0, 1.4128769301767186), (0, 1.4100645680800163), (0, 1.4072439348842531), (0, 1.404377106848198), (0, 1.4014492115547952), (0, 1.398472250661235), (0, 1.3954779125341072), (0, 1.392504226026756), (0, 1.3895834274701653), (0, 1.3867355231827763), (0, 1.3839676727792014), (0, 1.3812770794975917), (0, 1.3786549389125327), (0, 1.3760899389219394), (0, 1.3735707546716422), (0, 1.3710875483540683), (0, 1.368632708094288), (0, 1.3662010812871215), (0, 1.363789904044826), (0, 1.3613985575904197), (0, 1.3590282255960247), (0, 1.3566814847653328), (0, 1.3543618389180365), (0, 1.3520731996280044)]