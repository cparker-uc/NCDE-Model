Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=1024
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.0001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=1463.3494727611542
Loss over time=[(0, 13451.802693084739), (0, 13.251771522547939), (0, 14.24540708745297), (0, 15.281952089527904), (0, 16.183086709913653), (0, 16.911698608881803), (0, 17.47112969713864), (0, 17.876679367709183), (0, 18.146630389619578), (0, 18.299534807782337), (0, 18.353578153506337), (0, 18.326530612488458), (0, 18.235713131026476), (0, 18.09774630376276), (0, 17.92778825296988), (0, 17.72621262826933), (0, 17.57737050496317), (0, 17.38175522029574), (0, 17.210389885507748), (0, 17.05853832918826), (0, 16.933506456543732), (0, 16.814828188893546), (0, 16.723450319389055), (0, 16.650536365850602), (0, 16.59381161749773), (0, 16.550684938514035), (0, 16.520066465446728), (0, 16.496814259342933), (0, 16.481897283621063), (0, 16.472873804757675), (0, 16.468839655916245), (0, 16.467253325896653), (0, 16.468538828949487), (0, 16.471745698910677), (0, 16.476529152087213), (0, 16.48140795749063), (0, 16.487191353634252), (0, 16.493203403109977), (0, 16.499144714610736), (0, 16.50513048953977), (0, 16.511368416790756), (0, 16.516327080085535), (0, 16.52150098179254), (0, 16.526472087735826), (0, 16.531314808739737), (0, 16.53497728824407), (0, 16.538739611692744), (0, 16.54218212814758), (0, 16.545448497748342), (0, 16.551217665519065), (0, 16.550657133854518), (0, 16.55299538058532), (0, 16.555056045104056), (0, 16.556857080059057), (0, 16.558417525228723), (0, 16.55975601421125), (0, 16.560890456240994), (0, 16.561837892553974), (0, 16.56261441782286), (0, 16.563235137479257), (0, 16.56371416855603), (0, 16.56406464042266), (0, 16.56429872412365), (0, 16.56442766261115), (0, 16.564461815610755), (0, 16.56441069807303), (0, 16.564283035902584), (0, 16.56408680720923), (0, 16.563829294585293), (0, 16.5635171354257), (0, 16.56315635975547), (0, 16.562752448114924), (0, 16.562310362527686), (0, 16.56183459472444), (0, 16.561329202274063), (0, 16.560797845903057), (0, 16.56024382201652), (0, 16.55967009354322), (0, 16.55907932185012), (0, 16.55847389285705), (0, 16.557855941664574), (0, 16.55722737368337), (0, 16.55658989169528), (0, 16.5559450101436), (0, 16.555294073421795), (0, 16.55463827791375), (0, 16.55397868089525), (0, 16.553316214500647), (0, 16.552651701231937), (0, 16.551985862807086), (0, 16.551319334764006), (0, 16.5506526708236), (0, 16.54998635156233), (0, 16.5493207959224), (0, 16.548656366689798), (0, 16.547993374404026), (0, 16.547332085261726), (0, 16.546672724416116), (0, 16.54601548523247), (0, 16.545360526511804), (0, 16.544707981819638), (0, 16.544057959063746), (0, 16.54341054711115), (0, 16.542765814107554), (0, 16.542123813165933), (0, 16.541484583638706), (0, 16.540848153005026), (0, 16.540214537748167), (0, 16.539583744669372), (0, 16.538955772573175), (0, 16.5383306177832), (0, 16.537708266282788), (0, 16.537088701322315), (0, 16.536471902627504), (0, 16.535857845902328), (0, 16.535246505305192), (0, 16.53463785246652), (0, 16.53403185628807), (0, 16.533428487013097), (0, 16.532827710292114), (0, 16.532229496044977), (0, 16.5316338077077), (0, 16.531040614602276), (0, 16.530449882464307), (0, 16.529861575404357), (0, 16.529275663614662), (0, 16.52869211310205), (0, 16.528110891472352), (0, 16.527531964741787), (0, 16.52695530447055), (0, 16.52638087745175), (0, 16.52580865495396), (0, 16.525238603351763), (0, 16.524670696903996), (0, 16.52410490326569), (0, 16.523541197264276), (0, 16.52297954965588), (0, 16.52241993108536), (0, 16.521862316819277), (0, 16.521306679100256), (0, 16.520752992487527), (0, 16.52020123219903), (0, 16.519651367826846), (0, 16.519103381779257), (0, 16.518557243568214), (0, 16.5180129330642), (0, 16.51747042269181), (0, 16.5169296911117), (0, 16.51639071383816), (0, 16.51585346716382), (0, 16.51531793010468), (0, 16.51478407728145), (0, 16.51425188748859), (0, 16.5137213382097), (0, 16.513192408204958), (0, 16.51266507366253), (0, 16.512139313401438), (0, 16.5116151061786), (0, 16.511092429296237), (0, 16.51057126192909), (0, 16.51005158280269), (0, 16.50953337040751), (0, 16.50901660282059), (0, 16.5085012599479), (0, 16.50798731989804), (0, 16.507474762222543), (0, 16.50696356577879), (0, 16.50645370981658), (0, 16.505945173617857), (0, 16.505437935857756), (0, 16.504931977860206), (0, 16.50442727691465), (0, 16.503923813659483), (0, 16.503421568676963), (0, 16.50292052030746), (0, 16.502420647884726), (0, 16.50192193290191), (0, 16.501424353447153), (0, 16.50092789108639), (0, 16.50043252404193), (0, 16.499938234338238), (0, 16.49944500092407), (0, 16.498952804914452), (0, 16.498461625651164), (0, 16.497971442941502), (0, 16.497482239995197), (0, 16.496993994268227), (0, 16.496506687473296), (0, 16.49602030091923), (0, 16.495534813833164), (0, 16.495050209343997), (0, 16.494566465438623), (0, 16.494083565055956), (0, 16.49360148867365), (0, 16.493120216777754), (0, 16.492639730582678), (0, 16.492160013527013), (0, 16.491681043558927), (0, 16.491202804467225), (0, 16.4907252766151)]