Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=64
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=941.7259840965271
Loss over time=[(0, 15.070158341766183), (0, 13.685905545793894), (0, 12.778276688341686), (0, 11.092592107289589), (0, 7.613761490795238), (0, 3.5589587880343525), (0, 1.6415700958041757), (0, 1.2637904771850952), (0, 1.210556596687776), (0, 1.182563323604079), (0, 1.141924457579177), (0, 1.086016474650061), (0, 1.0164194098694435), (0, 0.9672456012227317), (0, 0.884999157143188), (0, 0.8325324183398588), (0, 0.777139227941486), (0, 0.7206999181662339), (0, 0.6648917711934694), (0, 0.6106074132608691), (0, 0.5623742123821477), (0, 0.5261857441966865), (0, 0.5091421531936136), (0, 0.5162412996828809), (0, 0.5302192479481792), (0, 0.5200536317679284), (0, 0.48929985745139143), (0, 0.5224839879085804), (0, 0.45992136200998096), (0, 0.4652017849964474), (0, 0.470430123606441), (0, 0.47355409442485735), (0, 0.47425463284777053), (0, 0.47265344648232727), (0, 0.4691177836093969), (0, 0.4640992048390507), (0, 0.45802607524729716), (0, 0.45126162847195633), (0, 0.4441038687351659), (0, 0.4367913040341816), (0, 0.42949830858087185), (0, 0.4223328846132062), (0, 0.4153572759021795), (0, 0.40863274288491364), (0, 0.40227088986994586), (0, 0.3964907085543725), (0, 0.39173837583955234), (0, 0.3889949430700748), (0, 0.3902274138648946), (0, 0.3963956906870591), (0, 0.39821721051800524), (0, 0.39292497803527787), (0, 0.38976908545140915), (0, 0.38929175202750216), (0, 0.38989562851819426), (0, 0.3906304744388356), (0, 0.39120311995227547), (0, 0.39147618033980747), (0, 0.3914267522242538), (0, 0.39108212828044225), (0, 0.39048776274246816), (0, 0.38965580651955345), (0, 0.38859728845496466), (0, 0.3874130143381144), (0, 0.38641577551057615), (0, 0.38596935991162035), (0, 0.3852939046508443), (0, 0.38384547074337627), (0, 0.3811625389747005), (0, 0.3719281738433078), (0, 0.3540401080278813), (0, 0.3479610974734518), (0, 0.3685915712848946), (0, 0.33799614902854297), (0, 0.3671041024021053), (0, 0.3506359836554766), (0, 0.35228103054323495), (0, 0.3334223613837816), (0, 0.3466281755802146), (0, 0.35511723150972957), (0, 0.3663396947150417), (0, 0.5097081802967337), (0, 0.4112660859407578), (0, 0.45216531212325056), (0, 0.49790538158900893), (0, 0.49578757779814775), (0, 0.5214567663063191), (0, 0.4776457004733947), (0, 0.5028955262285224), (0, 0.5097714876253924), (0, 0.5544887423433728), (0, 0.5528092894066801), (0, 0.5237099058042566), (0, 0.4850552899391503), (0, 0.4617386719602856), (0, 0.4600891909750253), (0, 0.45926495082715446), (0, 0.4547711290358007), (0, 0.4518632875572157), (0, 0.4502059660515004)]