Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=64
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.0001
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=1
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=15.28389024734497
Loss over time=[(0, 31.924829483032227), (0, 31.86441993713379), (0, 31.806955337524414), (0, 31.752090454101562), (0, 31.699310302734375), (0, 31.648117065429688), (0, 31.59814453125), (0, 31.549026489257812), (0, 31.500320434570312), (0, 31.450899124145508), (0, 31.39928436279297), (0, 31.343225479125977), (0, 31.285268783569336), (0, 31.22581672668457), (0, 31.164466857910156), (0, 31.10150146484375), (0, 31.03672981262207), (0, 30.969858169555664), (0, 30.898767471313477), (0, 30.82404136657715), (0, 30.745223999023438), (0, 30.661596298217773), (0, 30.572296142578125), (0, 30.476354598999023), (0, 30.371915817260742), (0, 30.25745391845703), (0, 30.133180618286133), (0, 29.999086380004883), (0, 29.85540008544922), (0, 29.702659606933594), (0, 29.541725158691406), (0, 29.37378692626953), (0, 29.200376510620117), (0, 29.022722244262695), (0, 28.84307861328125), (0, 28.66283416748047), (0, 28.485023498535156), (0, 28.31184196472168), (0, 28.14496612548828), (0, 27.98541259765625), (0, 27.833389282226562), (0, 27.688369750976562), (0, 27.549209594726562), (0, 27.414403915405273), (0, 27.282434463500977), (0, 27.15199851989746), (0, 27.022188186645508), (0, 26.89251708984375), (0, 26.762842178344727), (0, 26.631616592407227), (0, 26.500272750854492), (0, 26.369665145874023), (0, 26.240646362304688), (0, 26.114015579223633), (0, 25.98841094970703), (0, 25.865516662597656), (0, 25.745956420898438), (0, 25.629961013793945), (0, 25.51744842529297), (0, 25.408050537109375), (0, 25.301172256469727), (0, 25.196043014526367), (0, 25.091772079467773), (0, 24.987449645996094), (0, 24.882253646850586), (0, 24.775476455688477), (0, 24.666555404663086), (0, 24.555994033813477), (0, 24.444486618041992), (0, 24.332902908325195), (0, 24.222116470336914), (0, 24.11273765563965), (0, 24.00538444519043), (0, 23.900625228881836), (0, 23.79880714416504), (0, 23.700143814086914), (0, 23.60474395751953), (0, 23.51262855529785), (0, 23.42372703552246), (0, 23.337905883789062), (0, 23.254974365234375), (0, 23.174734115600586), (0, 23.09694480895996), (0, 23.021345138549805), (0, 22.947587966918945), (0, 22.875259399414062), (0, 22.803842544555664), (0, 22.732770919799805), (0, 22.66145896911621), (0, 22.589387893676758), (0, 22.516157150268555), (0, 22.44157600402832), (0, 22.365707397460938), (0, 22.288909912109375), (0, 22.21184730529785), (0, 22.13541603088379), (0, 22.060598373413086), (0, 21.988317489624023), (0, 21.91927146911621), (0, 21.85383415222168)]