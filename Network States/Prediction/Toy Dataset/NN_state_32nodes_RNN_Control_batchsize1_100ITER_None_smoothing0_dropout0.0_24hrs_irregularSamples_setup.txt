Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=32
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=683.7604689598083
Loss over time=[(0, 16.15546031501462), (0, 15.376275031461441), (0, 14.843817975909488), (0, 14.391265222552342), (0, 13.941382683875396), (0, 13.425698645368163), (0, 12.767294836511113), (0, 11.885097332958537), (0, 10.706032636543503), (0, 9.205338166203267), (0, 7.459681016238721), (0, 5.657396618671078), (0, 4.0366831528938505), (0, 2.777918226263305), (0, 1.9248677061861061), (0, 1.4056943925098764), (0, 1.1115383409687865), (0, 0.9518962925801117), (0, 0.8673788729564914), (0, 0.82291641051375), (0, 0.798858995199148), (0, 0.784574952890604), (0, 0.7744780020782804), (0, 0.7657320783265159), (0, 0.7569959700196781), (0, 0.7477375554897702), (0, 0.7378156677734284), (0, 0.7271742473883531), (0, 0.7156490025945274), (0, 0.7030596084263309), (0, 0.6894914490632594), (0, 0.6758127627780898), (0, 0.6664915328258635), (0, 0.6598627844548108), (0, 0.6300356913045566), (0, 0.613117030240112), (0, 0.5999275876115366), (0, 0.6062825429202172), (0, 0.5881867247897644), (0, 0.5891152863094727), (0, 0.5502595847045446), (0, 0.5486909908542804), (0, 0.55660787522264), (0, 0.547615209555706), (0, 0.5102488395574915), (0, 0.6747077780429178), (0, 0.646757162502047), (0, 0.5746351763766523), (0, 0.5978065744563286), (0, 0.5217156269108019), (0, 0.458994097911859), (0, 0.5278718276438993), (0, 0.48829421883811747), (0, 0.4395453943548948), (0, 0.4414954198861781), (0, 0.44741132037772674), (0, 0.44228894345781716), (0, 0.4412321683894709), (0, 0.439035629457501), (0, 0.4337769180363546), (0, 0.42290730026711715), (0, 0.4316420185487506), (0, 0.4228928384049262), (0, 0.4200393277657923), (0, 0.4236861636087233), (0, 0.4199111470918506), (0, 0.42520038652921927), (0, 0.4279691922878747), (0, 0.4268613946110044), (0, 0.42549468793784045), (0, 0.42374813450256105), (0, 0.4211894797146245), (0, 0.41954102019318956), (0, 0.41881767792576835), (0, 0.41790851094897796), (0, 0.41664131493218665), (0, 0.41532541748490687), (0, 0.4140408666929594), (0, 0.4126653859847392), (0, 0.41124506751615025), (0, 0.40988167579510426), (0, 0.40861509081926467), (0, 0.40742578159031323), (0, 0.4062781541406415), (0, 0.40515510351220985), (0, 0.4040623377960942), (0, 0.40301547920717923), (0, 0.40202485869536414), (0, 0.401086200471362), (0, 0.4001819885230977), (0, 0.39929350761305704), (0, 0.3984140502344303), (0, 0.3975506975752789), (0, 0.3967144593560454), (0, 0.39591068502348503), (0, 0.39513799502320507), (0, 0.39439353650201175), (0, 0.3936782656827793), (0, 0.3929982868079329), (0, 0.3923619349097467)]