Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=2048
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 5e-05
    maximize: False
    weight_decay: 1e-06
)Training Iterations=100
Learning rate=5e-05
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=358.0426926612854
Loss over time=[(0, 25.30270265304611), (0, 23.18055412665413), (0, 11.947176397897541), (0, 11.423597023928256), (0, 11.382258024394538), (0, 11.34556245326271), (0, 11.324825060534915), (0, 11.28591501796063), (0, 11.171401532911313), (0, 10.948570121574665), (0, 10.67756491712697), (0, 10.530054827116894), (0, 9.560223872360666), (0, 8.735517145053976), (0, 9.452822015427444), (0, 8.506493410942054), (0, 9.287247539151144), (0, 10.273494877238418), (0, 11.342623641527032), (0, 12.389486431315051), (0, 13.342808764624937), (0, 14.15971532097811), (0, 14.820656175476556), (0, 15.32739574124362), (0, 15.708211817055483), (0, 16.042227223777946), (0, 16.51922095648454), (0, 17.23174426710925), (0, 17.21954407202594), (0, 17.004441900038817), (0, 16.995535389435386), (0, 17.018335035471566), (0, 16.958701478524013), (0, 16.775830501782483), (0, 16.46529250929702), (0, 16.043128692648263), (0, 15.541213826843663), (0, 15.008150535716782), (0, 14.507786232099178), (0, 14.07938340512377), (0, 13.622744264094159), (0, 13.016900589298691), (0, 12.38942619882472), (0, 11.826570484937598), (0, 11.302904257989072), (0, 10.783923961858502), (0, 10.257368232102166), (0, 9.690232394124976), (0, 9.498180943912345), (0, 9.317805434800148), (0, 9.168023274881183), (0, 9.044388307532774), (0, 8.940349340565719), (0, 8.843551247122486), (0, 8.737659504180959), (0, 8.611628396195853), (0, 8.466169802084716), (0, 8.308372923158537), (0, 8.143171633032544), (0, 7.971177340414393), (0, 7.790971265699994), (0, 7.601467318396184), (0, 7.40296152165745), (0, 7.197202004119699), (0, 6.986993693981041), (0, 6.775642788415573), (0, 6.566383419368837), (0, 6.3618864022222095), (0, 6.163958368382252), (0, 5.973543459553753), (0, 5.791050207348456), (0, 5.616843246743146), (0, 5.4515990939185475), (0, 5.296325932306135), (0, 5.152133772235206), (0, 5.020055160227983), (0, 4.901297218358556), (0, 4.79906433832579), (0, 4.738433218157638), (0, 568.4183872232029), (0, 5.237366166864464), (0, 7.282615842236016), (0, 10.081095299661335), (0, 13.150976577547997), (0, 16.18431830591266), (0, 19.18900911018844), (0, 22.32758681913989), (0, 25.716401035043976), (0, 29.218473850339645), (0, 32.50909933488869), (0, 35.314061067687106), (0, 37.51555672355861), (0, 39.11554744778842), (0, 40.17030595780738), (0, 40.74996757076991), (0, 40.925081800309655), (0, 40.76848475752941), (0, 40.36194561064085), (0, 39.799635045037306), (0, 39.18274684535296)]