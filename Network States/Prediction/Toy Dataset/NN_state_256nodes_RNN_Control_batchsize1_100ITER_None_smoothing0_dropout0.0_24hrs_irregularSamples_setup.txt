Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=256
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=179.54740810394287
Loss over time=[(0, 63.29745575737971), (0, 14.112090749632364), (0, 13.490383176667201), (0, 13.137908243286356), (0, 12.453128184009984), (0, 11.387861458682117), (0, 9.96703056617286), (0, 8.025193679827135), (0, 6.329829564415085), (0, 5.03884650142141), (0, 3.9506323519827364), (0, 3.2490611934331075), (0, 2.8141129111935306), (0, 2.5916768326761397), (0, 2.4978821845117136), (0, 2.467545017310527), (0, 2.463020525539664), (0, 2.4666851692594105), (0, 2.4662608837269544), (0, 2.46234983992477), (0, 2.4523021653332595), (0, 2.4362149696991047), (0, 2.4147143135108555), (0, 2.388678509105935), (0, 2.3590879895457078), (0, 2.326948361489451), (0, 2.2932533869312284), (0, 2.2589704848889505), (0, 2.225039555866718), (0, 2.1923797721536378), (0, 2.1619004903564067), (0, 2.1345122506640575), (0, 2.1111320304528354), (0, 2.092673328228797), (0, 2.0800059722737965), (0, 2.073863806546198), (0, 2.0746748676378117), (0, 2.082299916950154), (0, 2.0957122285862577), (0, 2.112751482075419), (0, 2.1302023855982024), (0, 2.144436659464214), (0, 2.152540599494182), (0, 2.153361850849621), (0, 2.147793495130292), (0, 2.1381459300012415), (0, 2.1271009275420183), (0, 2.1168622887656823), (0, 2.1087784003864822), (0, 2.1033633915449808), (0, 2.100515890659207), (0, 2.099772581759667), (0, 2.100515814146146), (0, 2.1021147757120757), (0, 2.1040085683619947), (0, 2.1057473075085684), (0, 2.1070061146532404), (0, 2.107582796120259), (0, 2.107386318653981), (0, 2.1064204131571223), (0, 2.1047648128131784), (0, 2.102555499072571), (0, 2.0999647677373496), (0, 2.09718154422584), (0, 2.094392424260148), (0, 2.0917640079780493), (0, 2.089427416118645), (0, 2.087466246492862), (0, 2.0859094992537393), (0, 2.084730954048192), (0, 2.0838560052160857), (0, 2.0831758058721483), (0, 2.08256690342484), (0, 2.08191298818814), (0, 2.0811242810961743), (0, 2.0801504107233053), (0, 2.0789842261031364), (0, 2.0776564968217976), (0, 2.0762238144874847), (0, 2.074753508297564), (0, 2.073309372588953), (0, 2.071941082229238), (0, 2.07067856321092), (0, 2.0695311969737453), (0, 2.0684907546741496), (0, 2.0675365569470996), (0, 2.0666414223467044), (0, 2.0657772306548545), (0, 2.064919363090844), (0, 2.0640496567668816), (0, 2.0631578014664655), (0, 2.062241314194242), (0, 2.061304425221061), (0, 2.060356181555099), (0, 2.059408180316788), (0, 2.0584723082582355), (0, 2.0575587373637947), (0, 2.056674519170669), (0, 2.055822817375193), (0, 2.0550028800561186)]