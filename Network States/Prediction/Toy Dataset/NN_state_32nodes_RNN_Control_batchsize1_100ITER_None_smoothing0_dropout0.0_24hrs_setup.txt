Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=32
Output channels=1

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.0001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=1
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=210.9851188659668
Loss over time=[(0, 32.928306579589844), (0, 32.02312088012695), (0, 31.83551025390625), (0, 31.72791862487793), (0, 31.64552116394043), (0, 31.574560165405273), (0, 31.509994506835938), (0, 31.449556350708008), (0, 31.392107009887695), (0, 31.33684539794922), (0, 31.283416748046875), (0, 31.23150634765625), (0, 31.180944442749023), (0, 31.1314697265625), (0, 31.08254051208496), (0, 31.03369903564453), (0, 30.98493766784668), (0, 30.936445236206055), (0, 30.88832664489746), (0, 30.840553283691406), (0, 30.793045043945312), (0, 30.745697021484375), (0, 30.69843292236328), (0, 30.651182174682617), (0, 30.60392189025879), (0, 30.556617736816406), (0, 30.50911521911621), (0, 30.461524963378906), (0, 30.41377067565918), (0, 30.365827560424805), (0, 30.31768226623535), (0, 30.26933479309082), (0, 30.220815658569336), (0, 30.172210693359375), (0, 30.124114990234375), (0, 30.11509895324707), (0, 30.030370712280273), (0, 29.98267364501953), (0, 29.93564796447754), (0, 29.88840675354004), (0, 29.840805053710938), (0, 29.79286003112793), (0, 29.74454689025879), (0, 29.695884704589844), (0, 29.647226333618164), (0, 29.598281860351562), (0, 29.549428939819336), (0, 29.501523971557617), (0, 29.45224952697754), (0, 29.408523559570312), (0, 29.357149124145508), (0, 29.31005096435547), (0, 29.262609481811523), (0, 29.21484375), (0, 29.166845321655273), (0, 29.118566513061523), (0, 29.06988525390625), (0, 29.021753311157227), (0, 28.973724365234375), (0, 28.92503547668457), (0, 28.875890731811523), (0, 28.826322555541992), (0, 28.77642250061035), (0, 28.7262020111084), (0, 28.67561149597168), (0, 28.624717712402344), (0, 28.573556900024414), (0, 28.52208137512207), (0, 28.47038459777832), (0, 28.418420791625977), (0, 28.3663330078125), (0, 28.314302444458008), (0, 28.263959884643555), (0, 28.208654403686523), (0, 28.156204223632812), (0, 28.10204315185547), (0, 28.04767608642578), (0, 27.993837356567383), (0, 27.9392147064209), (0, 27.8844051361084), (0, 27.829256057739258), (0, 27.77376365661621), (0, 27.717920303344727), (0, 27.66175079345703), (0, 27.605262756347656), (0, 27.548479080200195), (0, 27.491422653198242), (0, 27.43410301208496), (0, 27.376550674438477), (0, 27.318817138671875), (0, 27.26108741760254), (0, 27.202743530273438), (0, 27.1445255279541), (0, 27.08608055114746), (0, 27.027456283569336), (0, 26.96877098083496), (0, 26.910303115844727), (0, 26.855257034301758), (0, 26.79395866394043), (0, 26.735109329223633)]