Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=32
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=12.910839796066284
Loss over time=[(0, 14.751355918698458), (0, 37.909010521805214), (0, 16.351325438949114), (0, 15.015450091706859), (0, 15.377310042623362), (0, 21.397798761621488), (0, 21.96568557643609), (0, 16.31203301450884), (0, 14.325358813564426), (0, 16.0136770336478), (0, 13.999239971962252), (0, 14.058869659531842), (0, 14.124360604926844), (0, 14.27098322072508), (0, 14.050881265456988), (0, 20.638244133422333), (0, 13.864897096991031), (0, 13.760356769238397), (0, 22.548873569500127), (0, 14.028551762826192), (0, 13.541443706782637), (0, 13.808941302005953), (0, 13.927741446279937), (0, 14.441237788699096), (0, 25.38412030647225), (0, 13.893988606082283), (0, 14.087952352426656), (0, 13.638586833858517), (0, 13.847608829830984), (0, 13.71028068362144), (0, 15.007705063433216), (0, 13.746807024974622), (0, 13.727992776132195), (0, 18.396474871717697), (0, 13.869351436341367), (0, 13.817341131720648), (0, 13.679445407087378), (0, 14.011173677403898), (0, 14.12562444191054), (0, 13.86524963179314), (0, 13.787637767742334), (0, 13.730174474538083), (0, 13.667299104064076), (0, 13.612980199937612), (0, 15.90142273613538), (0, 13.719607440662223), (0, 13.728377290958113), (0, 13.722431884346413), (0, 13.71016267147995), (0, 13.696583073350979), (0, 13.68270713440466), (0, 13.66887453401248), (0, 13.658480392048828), (0, 13.642059619115086), (0, 13.628087855047191), (0, 13.61510205949918), (0, 13.602370259267401), (0, 13.590138758317877), (0, 13.578936623598386), (0, 13.57095706548676), (0, 13.56850127590227), (0, 13.570967692996378), (0, 13.544579157364607), (0, 13.522146024799442), (0, 13.514068625765153), (0, 13.492362659831342), (0, 13.47827094870115), (0, 13.465441059168642), (0, 13.4530560051725), (0, 13.440901493385539), (0, 13.428895575890119), (0, 13.41700114260918), (0, 13.405200123495158), (0, 13.393483653963239), (0, 13.381847712772867), (0, 13.370290892478819), (0, 13.358813262410706), (0, 13.347415727683464), (0, 13.336099612265883), (0, 13.324866429192776), (0, 13.313717728655023), (0, 13.302655034094442), (0, 13.291679780524133), (0, 13.280793242268489), (0, 13.269996563596273), (0, 13.259290738053423), (0, 13.248676562817693), (0, 13.238154699109211), (0, 13.227725658614565), (0, 13.21738979940335), (0, 13.20714730249671), (0, 13.196998277241343), (0, 13.18694267478332), (0, 13.17698031449131), (0, 13.167110956967448), (0, 13.157334217318429), (0, 13.147649652086029), (0, 13.138056738643579), (0, 13.128554873938102), (0, 13.119143407017168), (0, 13.109821597921956), (0, 13.100588713885776), (0, 13.091443931880825), (0, 13.082386396744738), (0, 13.073415268585208), (0, 13.064529627787072), (0, 13.055728573386979), (0, 13.047011168936812), (0, 13.03837648484742), (0, 13.029823554714726), (0, 13.021351434868995), (0, 13.01295916925765), (0, 13.004645775779634), (0, 12.996410340302504), (0, 12.988251897960055), (0, 12.980169493210221), (0, 12.97216219248624), (0, 12.964229085665409), (0, 12.956369255701746), (0, 12.94858177394746), (0, 12.940865753238448), (0, 12.933220326095606), (0, 12.925644617625617), (0, 12.918137758328536), (0, 12.910698912427433), (0, 12.90332726722374), (0, 12.896021977133055), (0, 12.888782263641728), (0, 12.881607316579705), (0, 12.874496387479374), (0, 12.867448702560539), (0, 12.860463522590097), (0, 12.85354011588811), (0, 12.846677749994257), (0, 12.839875735689096), (0, 12.833133374131009), (0, 12.826449983511862), (0, 12.819824896336689), (0, 12.813257455875652), (0, 12.806747029584857), (0, 12.800292973259209), (0, 12.793894675045982), (0, 12.787551519531187), (0, 12.781262906387532), (0, 12.77502825876873), (0, 12.768846979627265), (0, 12.762718510391695), (0, 12.756642279289522), (0, 12.750617753541462), (0, 12.744644362403749), (0, 12.73872158889767), (0, 12.732848904975716), (0, 12.727025772808574), (0, 12.721251690500713), (0, 12.715526136873335), (0, 12.709848617948808), (0, 12.70421865110067), (0, 12.698635720125877), (0, 12.693099361208162), (0, 12.687609100602776), (0, 12.682164463345368), (0, 12.676764967588305), (0, 12.671410185978703), (0, 12.666099660153156), (0, 12.660832937986589), (0, 12.655609600541753), (0, 12.650429184043723), (0, 12.645291314326906), (0, 12.640195577902825), (0, 12.635141525313244), (0, 12.630128802550615), (0, 12.625157000001913), (0, 12.62022573376362), (0, 12.615334649744126), (0, 12.61048337162964), (0, 12.605671541670977), (0, 12.600898799245831), (0, 12.596164792993065), (0, 12.591469212956511), (0, 12.586811683982608), (0, 12.582191900799996), (0, 12.577609560607407), (0, 12.57306428639164), (0, 12.568555818309962), (0, 12.564083796520803), (0, 12.55964795934249), (0, 12.555247975742232), (0, 12.550883554671179), (0, 12.54655438408376), (0, 12.542260185972488), (0, 12.538000673036427), (0, 12.533775566769549), (0, 12.529584544774067), (0, 12.525427339347557), (0, 12.521303693081746), (0, 12.517213309165797), (0, 12.513155924968984), (0, 12.509131252797387), (0, 12.505139003616032), (0, 12.50117895444567)]