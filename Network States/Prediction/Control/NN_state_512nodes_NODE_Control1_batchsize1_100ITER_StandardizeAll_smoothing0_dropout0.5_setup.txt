Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=2
Hidden channels=512
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.003
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.003
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=StandardizeAll
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=86.89403700828552
Loss over time=[(0, 6.434693277869852), (0, 6.150124926305481), (0, 6.133066303454546), (0, 6.127332228823221), (0, 6.126565837293895), (0, 6.12955771260439), (0, 6.126348216454368), (0, 6.127067037439218), (0, 6.126348478042889), (0, 6.125622901647376), (0, 6.126344613547178), (0, 6.125318774574844), (0, 6.126088214958239), (0, 6.124834995222505), (0, 6.1256521175175855), (0, 6.124719294705472), (0, 6.125345882551303), (0, 6.124462890229719), (0, 6.124967081218187), (0, 6.12433944106369), (0, 6.124664663673977), (0, 6.1241293119387), (0, 6.124353574278283), (0, 6.12398650299726), (0, 6.124067272458074), (0, 6.123789262082894), (0, 6.123802813458346), (0, 6.123625343273161), (0, 6.12353993764199), (0, 6.12343116426219), (0, 6.123307242954639), (0, 6.123248668060773), (0, 6.123072828177547), (0, 6.12305254305646), (0, 6.122864145137518), (0, 6.122857485427574), (0, 6.122660375766198), (0, 6.122657683854976), (0, 6.12247298540598), (0, 6.12245893266365), (0, 6.122297134044323), (0, 6.122260273002922), (0, 6.122128604993225), (0, 6.12207017330764), (0, 6.121971268443769), (0, 6.121886579282298), (0, 6.121815101056233), (0, 6.121719123016145), (0, 6.121665367357426), (0, 6.121565565176066), (0, 6.121516113290799), (0, 6.121427377284494), (0, 6.121374498383932), (0, 6.1213028294364795), (0, 6.121240333881112), (0, 6.121185491176269), (0, 6.121120288810133), (0, 6.121075818479429), (0, 6.121014034969904), (0, 6.120970713872432), (0, 6.120919723278672), (0, 6.120874943507283), (0, 6.120834893831392), (0, 6.120789363146901), (0, 6.120755276212859), (0, 6.120714907304758), (0, 6.1206823725363115), (0, 6.120649479693889), (0, 6.120616579435296), (0, 6.120589521108219), (0, 6.120559298907432), (0, 6.120534757386095), (0, 6.120509237733401), (0, 6.120485043560226), (0, 6.120463902703812), (0, 6.120441379067108), (0, 6.120422508476108), (0, 6.120402964593509), (0, 6.120384701273909), (0, 6.12036815259215), (0, 6.120351030460554), (0, 6.12033619081333), (0, 6.120320969306655), (0, 6.120306718437652), (0, 6.1202934613418885), (0, 6.120279987855881), (0, 6.12026792838206), (0, 6.12025564588094), (0, 6.120244065055644), (0, 6.12023303707287), (0, 6.120221962287754), (0, 6.120211751480249), (0, 6.120201391015687), (0, 6.120191544605637), (0, 6.1201819629238265), (0, 6.120172428150172), (0, 6.120163397413563), (0, 6.1201542601368), (0, 6.120145502861683), (0, 6.120136824780104)]