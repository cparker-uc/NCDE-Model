Model Setup for {PATIENT_GROUPS} #{INDIVIDUAL_NUMBER} Trained Network:

NCDE Network Architecture Parameters
Input channels=3
Hidden channels=16
Output channels=3

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Training Iterations=200
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=None
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=0
Label smoothing factor=0
Test Patient Combinations:
Training batch size=1
Training Results:
Runtime=210.457524061203
Loss over time=[(0, 44.0470771410177), (0, 40.898777761668946), (0, 38.00846870369415), (0, 35.91902252285894), (0, 33.07982809088596), (0, 31.78925600246247), (0, 30.993593073268116), (0, 29.90850789346236), (0, 28.841769573768826), (0, 27.960490063089168), (0, 27.154985214926835), (0, 26.309648822009446), (0, 25.587326995695182), (0, 25.084982524478978), (0, 24.564869674636554), (0, 24.076811703635883), (0, 23.485963703559843), (0, 22.680854861807802), (0, 21.732818677038935), (0, 21.68270597414018), (0, 20.798978282569262), (0, 19.458538235382804), (0, 18.566810306422372), (0, 17.853378434094246), (0, 17.417253404516014), (0, 16.934490536906356), (0, 16.18128966019356), (0, 15.238659936137703), (0, 14.534440928291147), (0, 13.847525478855436), (0, 13.524444403720393), (0, 13.48197351909937), (0, 13.604254622090494), (0, 14.062994145985295), (0, 14.669034877608835), (0, 15.109326751822001), (0, 14.779579797857584), (0, 14.263024900421186), (0, 13.925560085863667), (0, 13.182646718310911), (0, 12.631019105779544), (0, 12.388026885654101), (0, 12.421318337216974), (0, 12.444664219509287), (0, 12.617885617554087), (0, 11.980671747331845), (0, 11.609215846620911), (0, 11.389366030059819), (0, 11.465861438446412), (0, 11.13111595398906), (0, 11.264271709455222), (0, 11.383445470485754), (0, 11.264687090733903), (0, 11.123006831342407), (0, 11.732867333548384), (0, 11.459175158001102), (0, 11.687668255370026), (0, 11.529931621751352), (0, 11.027270986438955), (0, 10.240092217308584), (0, 10.298666420397339), (0, 9.859532779139403), (0, 9.42028878926984), (0, 9.448046001918355), (0, 9.616409824418318), (0, 9.625554291714145), (0, 9.893868932993945), (0, 9.684942896397922), (0, 9.477738408349502), (0, 9.494276407449787), (0, 9.5619985325088), (0, 9.84959281020217), (0, 9.316562024000186), (0, 8.67827653965733), (0, 8.160956194011087), (0, 8.280800463895861), (0, 7.8231821769816925), (0, 7.972390600700816), (0, 8.013460111652302), (0, 7.883012030779959), (0, 7.580561912908824), (0, 7.283276664358207), (0, 7.3296216379295345), (0, 7.8352005143547325), (0, 7.586554025629624), (0, 7.948799631053329), (0, 7.874273040862579), (0, 8.569825993025091), (0, 7.8734194108203), (0, 7.101401298524706), (0, 6.852007773489383), (0, 6.839831252284604), (0, 6.602464429496653), (0, 7.365158179202493), (0, 7.617711360631234), (0, 7.532195247437329), (0, 7.300579479871189), (0, 7.07343985828997), (0, 7.094000543195818), (0, 6.871887650750058), (0, 6.746532501124158), (0, 7.226280559771602), (0, 7.495092150135341), (0, 7.275601600902839), (0, 7.034750013607134), (0, 7.178632170283437), (0, 7.468847918446801), (0, 7.534657264722348), (0, 7.65804536984817), (0, 7.543814523290187), (0, 7.37709632868953), (0, 7.283509796100366), (0, 7.13945302666626), (0, 7.19508969911793), (0, 7.274916572019518), (0, 7.480326190234738), (0, 7.698958648796231), (0, 7.7466279176756885), (0, 7.743540269541223), (0, 7.74805430095144), (0, 7.809875699496177), (0, 7.828629532799238), (0, 7.8214623553013745), (0, 7.564351486810989), (0, 7.413207845972252), (0, 7.435638407985903), (0, 7.967854207249375), (0, 7.873258889408799), (0, 8.376657361556434), (0, 8.040824747593335), (0, 8.730095478787026), (0, 8.918003977094003), (0, 8.825049373095744), (0, 8.675404009898195), (0, 8.572910036998652), (0, 8.374873114952495), (0, 8.188273323291908), (0, 8.052207489099336), (0, 7.895586409744825), (0, 7.825562599097185), (0, 7.848083874909379), (0, 8.044059444947242), (0, 8.256815461737583), (0, 8.327863373381563), (0, 8.207848979110281), (0, 8.07779815992844), (0, 8.179474089440511), (0, 7.933681058321215), (0, 8.16845456091138), (0, 8.308922678668923), (0, 8.323617626514276), (0, 8.593545751358583), (0, 8.48055703277427), (0, 8.504814447912288), (0, 8.479847998508744), (0, 8.172718797461451), (0, 8.367020258475685), (0, 8.100840556446974), (0, 8.155996551797648), (0, 8.156523127448452), (0, 8.046381936021913), (0, 8.103177905702852), (0, 8.086279673598625), (0, 8.077156544848329), (0, 8.136698318569222), (0, 8.246273023013659), (0, 8.310945636157578), (0, 8.306176853512284), (0, 8.210283709111264), (0, 8.165870268884651), (0, 8.109124686115235), (0, 8.180530592973902), (0, 8.216131855595378), (0, 8.280097859670327), (0, 8.21043573555776), (0, 8.166380136220328), (0, 8.281032341043689), (0, 8.539412910199209), (0, 8.547363314684274), (0, 8.786098926434265), (0, 8.549053987638287), (0, 8.7339104023533), (0, 8.714735390400929), (0, 8.729831896275536), (0, 8.587787153707422), (0, 8.651355578478054), (0, 8.72625428783496), (0, 8.576827329897256), (0, 8.753316908645145), (0, 8.724667623010786), (0, 8.566575553663927), (0, 8.763088601803656), (0, 8.72415114404477), (0, 8.933286019134037), (0, 8.935184064696557), (0, 8.960594742572306), (0, 8.936334243459006), (0, 8.883540411085091), (0, 8.941036432651071), (0, 8.843379348634729)]