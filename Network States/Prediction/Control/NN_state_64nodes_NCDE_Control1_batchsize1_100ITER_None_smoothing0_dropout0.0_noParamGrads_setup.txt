Model Setup for {PATIENT_GROUPS} #{INDIVIDUAL_NUMBER} Trained Network:

NCDE Network Architecture Parameters
Input channels=3
Hidden channels=64
Output channels=3

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=None
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=0
Label smoothing factor=0
Test Patient Combinations:
Training batch size=1
Training Results:
Runtime=164.55060195922852
Loss over time=[(0, 59.829996974251095), (0, 54.34454609193767), (0, 44.31165934476318), (0, 37.363058775273714), (0, 30.557733785692193), (0, 25.759234907889436), (0, 17.543712945125424), (0, 13.270013012976927), (0, 11.583972780260439), (0, 11.714245527349592), (0, 14.773705606149235), (0, 13.24058633304953), (0, 15.567919812922662), (0, 16.632586320733378), (0, 14.27698682349831), (0, 14.262749499201556), (0, 14.894569210122164), (0, 12.556421879355677), (0, 12.478969657305996), (0, 9.894675509895892), (0, 9.599604944831432), (0, 8.222339476405557), (0, 7.071648527312327), (0, 5.968257952407936), (0, 7.874368585611935), (0, 7.062887300993519), (0, 6.206585473059563), (0, 5.86378203159677), (0, 5.188776720005236), (0, 5.102262452181214), (0, 5.094415905849318), (0, 5.221336285412282), (0, 5.37818197183707), (0, 5.021274322779912), (0, 6.464975158734863), (0, 5.314284905388118), (0, 5.686176630795714), (0, 5.835420542623159), (0, 5.1806259037528894), (0, 5.103913121596442), (0, 5.162479206465936), (0, 5.193620043839213), (0, 5.575683454039997), (0, 5.306769285835717), (0, 5.596537440543856), (0, 5.583076953206425), (0, 5.8751853034887445), (0, 5.809362499612018), (0, 5.776402898806913), (0, 5.708244874869386), (0, 5.897401660376059), (0, 5.789481382071205), (0, 5.835891636983824), (0, 6.283255047960388), (0, 6.237563613181796), (0, 6.21066055143271), (0, 6.217994392500636), (0, 6.5551383671793415), (0, 6.459099911317896), (0, 6.632730989131197), (0, 6.742236597007188), (0, 7.302564184902576), (0, 6.9432331166452), (0, 7.143094605723125), (0, 7.258184155867628), (0, 7.465826200120177), (0, 7.471981627717905), (0, 8.132366102477699), (0, 7.526649552771789), (0, 8.105057471450118), (0, 8.066358226130992), (0, 8.135922314816968), (0, 8.95401467044343), (0, 8.74144416853872), (0, 8.546467014138384), (0, 8.3848316354997), (0, 8.415946576986913), (0, 8.77197607593599), (0, 8.155214865673127), (0, 10.592081371066234), (0, 8.997827861317333), (0, 8.830247931497551), (0, 8.555066176260471), (0, 10.196492314421631), (0, 8.711252866757674), (0, 8.633324100255408), (0, 8.48518192832239), (0, 8.344890341530755), (0, 9.364482975014385), (0, 9.960735322573083), (0, 9.789918442347092), (0, 9.009951727053107), (0, 9.069806256766972), (0, 9.079611593584692), (0, 9.059668332759154), (0, 9.060212597409837), (0, 9.793490753099796), (0, 9.643956462468184), (0, 8.967720986391962), (0, 8.642854992658597)]