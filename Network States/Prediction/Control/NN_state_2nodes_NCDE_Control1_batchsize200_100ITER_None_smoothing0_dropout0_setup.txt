Model Setup for {PATIENT_GROUPS} Trained Network:

NCDE Network Architecture Parameters
Input channels=2
Hidden channels=2
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=100
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=200
Training Results:
Runtime=138.7596139907837
Loss over time=[(0, 50.43408833570653), (0, 49.860163146674374), (0, 49.23101769995899), (0, 48.54250747343632), (0, 47.79273455443142), (0, 46.979944547871476), (0, 46.09675997258177), (0, 45.12561851152694), (0, 44.03674639728103), (0, 42.786836853610446), (0, 41.31632391918602), (0, 39.541489745536076), (0, 37.33459868812576), (0, 34.53931135807), (0, 31.044307672734423), (0, 27.483568850571046), (0, 26.153866351032107), (0, 25.680421945260118), (0, 25.319451894360647), (0, 24.98860397223284), (0, 24.687044466417237), (0, 24.418126101965242), (0, 24.185180312347928), (0, 23.965561665360227), (0, 23.76610055846422), (0, 23.585658065336997), (0, 23.42159004409558), (0, 23.271422144515387), (0, 23.132843209739324), (0, 23.0037761513059), (0, 22.88234992113874), (0, 22.76684611382593), (0, 22.65566219680488), (0, 22.547291646384163), (0, 22.441488996668557), (0, 22.34052765918204), (0, 22.24429598127977), (0, 22.147472791706313), (0, 22.057541695476495), (0, 21.9711014195705), (0, 21.881976323139625), (0, 21.782789720446104), (0, 21.665036606332556), (0, 21.49170917286808), (0, 21.22284564337484), (0, 20.991313906120908), (0, 20.4232404496833), (0, 18.885698582706535), (0, 18.77192862629351), (0, 19.04153550733023), (0, 19.12737586330702), (0, 19.104327510907638), (0, 18.971804020899143), (0, 18.698193873002857), (0, 18.193300517394206), (0, 18.258932814583655), (0, 18.279171465073095), (0, 18.2746820487933), (0, 18.241785994749566), (0, 18.18560273746024), (0, 18.11546751616062), (0, 18.037931311896696), (0, 17.955506516322806), (0, 17.883130916886262), (0, 17.808681699236118), (0, 18.04249308387438), (0, 17.82444492853917), (0, 17.73795605602928), (0, 17.734135199983935), (0, 17.724246766728665), (0, 17.709104290399438), (0, 17.68893788644688), (0, 17.6638337528288), (0, 17.633822341867752), (0, 17.598915735854025), (0, 17.559202740169), (0, 17.514865182138838), (0, 17.46612119326471), (0, 17.413208467230454), (0, 17.356230719606877), (0, 17.295138230061806), (0, 17.244899297579483), (0, 17.23288139249081), (0, 17.217530796775847), (0, 17.198945197067616), (0, 17.177356196569942), (0, 17.153075805514497), (0, 17.126462836257385), (0, 17.097897689599247), (0, 17.067764321165686), (0, 17.0364367732406), (0, 17.00427669343434), (0, 16.97162569953365), (0, 16.938812622850435), (0, 16.906178421611415), (0, 16.87411667345216), (0, 16.84288756655116), (0, 16.81217728267279), (0, 16.782021515380233), (0, 16.752458888975482)]