Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=2
Hidden channels=128
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.006
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.006
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=StandardizeAll
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=16.12684202194214
Loss over time=[(0, 1.954281729163778), (0, 1.057003673134295), (0, 1.0115884678202591), (0, 1.0040471364927965), (0, 1.005831736835182), (0, 1.0041690937302716), (0, 1.0032760165064458), (0, 1.005212542176409), (0, 1.0029842377501386), (0, 1.0036726734592365), (0, 1.0042137330972982), (0, 1.0025696070406886), (0, 1.0038988176449593), (0, 1.0035107109165018), (0, 1.0026060427451529), (0, 1.003746591186809), (0, 1.0029393906798596), (0, 1.002815778780458), (0, 1.0034530879390335), (0, 1.0027244839176934), (0, 1.0028730224130162), (0, 1.0031765807226345), (0, 1.0025626028096968), (0, 1.002969630383462), (0, 1.0028887498582901), (0, 1.0025591749688283), (0, 1.0029473816072951), (0, 1.002635618826522), (0, 1.0026521741457224), (0, 1.0028140607012084), (0, 1.0025225109255618), (0, 1.0027125926392022), (0, 1.0026272226562136), (0, 1.0025323082175501), (0, 1.0026803953892898), (0, 1.0024946998187179), (0, 1.0025884592705856), (0, 1.0025506614503057), (0, 1.00248215655666), (0, 1.0025697893204049), (0, 1.0024469510202378), (0, 1.0025200043913534), (0, 1.0024621478076614), (0, 1.0024516738085671), (0, 1.0024746059885925), (0, 1.0024058066997767), (0, 1.0024583953609458), (0, 1.0023896464941369), (0, 1.002421939751228), (0, 1.0023860456713425), (0, 1.0023835758835733), (0, 1.0023789237763603), (0, 1.0023530065526352), (0, 1.0023653373998713), (0, 1.0023293174504853), (0, 1.0023463937732435), (0, 1.0023111838698622), (0, 1.0023242895811446), (0, 1.002295046929997), (0, 1.0023024341003601), (0, 1.0022788721414821), (0, 1.0022811424390157), (0, 1.002262523455541), (0, 1.0022611110218465), (0, 1.0022452906022734), (0, 1.0022420185205088), (0, 1.0022278652926813), (0, 1.0022236813148222), (0, 1.0022100306800201), (0, 1.0022058029597767), (0, 1.0021923125496195), (0, 1.0021882265699977), (0, 1.00217464843681), (0, 1.0021706320712533), (0, 1.00215736868458), (0, 1.002152984817964), (0, 1.0021405246937496), (0, 1.002135036564435), (0, 1.0021241211743122), (0, 1.0021169562337577), (0, 1.0021080232464132), (0, 1.0020989989533955), (0, 1.0020917771818467), (0, 1.0020815818284439), (0, 1.0020749690623474), (0, 1.0020650130880477), (0, 1.0020575810608259), (0, 1.0020490227987269), (0, 1.0020401607828688), (0, 1.0020328178408917), (0, 1.0020234750031292), (0, 1.0020158969665067), (0, 1.0020075177747687), (0, 1.0019988499908041), (0, 1.0019913330621557), (0, 1.0019825901925161), (0, 1.0019745333394268), (0, 1.001966723381578), (0, 1.0019581075156105), (0, 1.001950274756963), (0, 1.001942329391321), (0, 1.001933909799118), (0, 1.001926109949766), (0, 1.0019182060500436), (0, 1.001909938162756), (0, 1.0019021016174694), (0, 1.0018943258052067), (0, 1.0018862183617907), (0, 1.0018783048014324), (0, 1.0018706295319686), (0, 1.001862767508219), (0, 1.001854830011551), (0, 1.0018471063540293), (0, 1.0018394813118214), (0, 1.0018317498205753), (0, 1.0018239726069322), (0, 1.0018163222971677), (0, 1.0018087951764785), (0, 1.001801272838157), (0, 1.0017937143690605), (0, 1.0017861533693981), (0, 1.0017786578458046), (0, 1.0017712464985473), (0, 1.001763907432836), (0, 1.0017566285421546), (0, 1.0017493968942146), (0, 1.0017422383540007), (0, 1.0017351747917862), (0, 1.0017283194353392), (0, 1.001721870281258), (0, 1.0017165556242638), (0, 1.0017142673033548), (0, 1.001722344348083), (0, 1.0017625879171912), (0, 1.0019354437214498), (0, 1.0024689473099089), (0, 1.0045114785715947), (0, 1.0048886214256294), (0, 1.0055222902238463), (0, 1.001686893242869), (0, 1.0048782115027304), (0, 1.007574993486694), (0, 1.0023001682322448), (0, 1.0107374966999132), (0, 1.005227705068245), (0, 1.0070989023338297), (0, 1.0030647098674967), (0, 1.0053058253402862), (0, 1.0038401431873145), (0, 1.0051680545371762), (0, 1.002098109417628), (0, 1.0050842918907306), (0, 1.0020521201523052), (0, 1.0044303488074677), (0, 1.0019081140227517), (0, 1.0038841248580466), (0, 1.0021023012556025), (0, 1.002934411126458), (0, 1.0028040809490408), (0, 1.0020250108472906), (0, 1.003026320587881), (0, 1.0018228413075654), (0, 1.002768756632385), (0, 1.002031414638558), (0, 1.0022848652873124), (0, 1.0022821653941931), (0, 1.0019409053776442), (0, 1.0023930279409485), (0, 1.0018326256033965), (0, 1.0022507914474241), (0, 1.0019418554828514), (0, 1.002005430872345), (0, 1.0020916936335464), (0, 1.001830866955955), (0, 1.0021042032860639), (0, 1.0018215776544688), (0, 1.0019966923676853), (0, 1.00189698851153), (0, 1.0018691843835599), (0, 1.0019445911113842), (0, 1.0018052678354334), (0, 1.0019335027958878), (0, 1.0018001786879684), (0, 1.0018803747880172), (0, 1.0018224821031663), (0, 1.0018213733637986), (0, 1.0018423650575292), (0, 1.001780009523718), (0, 1.0018384794053836), (0, 1.001766093840401), (0, 1.0018150557247365), (0, 1.0017670722273104), (0, 1.0017858806193156), (0, 1.0017692994023355), (0, 1.001760465218274), (0, 1.001768144105595), (0, 1.0017412942731536), (0, 1.001760481900372), (0, 1.0017295759855591), (0, 1.0017481452541976)]