Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=2
Hidden channels=32
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.003
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.003
Weight decay=0.0
Optimizer reset frequency=200

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=StandardizeAll
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=12.79227328300476
Loss over time=[(0, 6.650871262541134), (0, 6.351768289498124), (0, 6.26284374038236), (0, 6.230354325012717), (0, 6.198480845163274), (0, 6.16719996490393), (0, 6.146964816434055), (0, 6.141726794893617), (0, 6.143983346523999), (0, 6.145978320170408), (0, 6.145453046777618), (0, 6.142755188629514), (0, 6.138720828023891), (0, 6.1342243878936635), (0, 6.130190294130611), (0, 6.127523232420779), (0, 6.126793312821536), (0, 6.127791358667749), (0, 6.12942950259361), (0, 6.130411797965322), (0, 6.130120651715589), (0, 6.12882317936772), (0, 6.127265116973482), (0, 6.126147371175789), (0, 6.1257911041080355), (0, 6.126092295150798), (0, 6.126698656884243), (0, 6.127239510718979), (0, 6.127473934529077), (0, 6.127332133124289), (0, 6.126891382779505), (0, 6.126325007723119), (0, 6.125837778537379), (0, 6.125591390143331), (0, 6.125636012406518), (0, 6.12588340876108), (0, 6.126153109291965), (0, 6.126276398451145), (0, 6.126189714525816), (0, 6.125954164084206), (0, 6.125699086781183), (0, 6.125539115483256), (0, 6.125518236358548), (0, 6.125604210439319), (0, 6.125722184016761), (0, 6.125799378512884), (0, 6.125797176220893), (0, 6.125721113918271), (0, 6.1256108030683345), (0, 6.125517243741564), (0, 6.125477352678526), (0, 6.125496687972459), (0, 6.125549184666396), (0, 6.125594555498066), (0, 6.125602836379136), (0, 6.125570321137749), (0, 6.125517592557739), (0, 6.125473362905382), (0, 6.125456497324271), (0, 6.1254672907393966), (0, 6.125491003290161), (0, 6.125508882762603), (0, 6.125509047972961), (0, 6.125491623922207), (0, 6.125466653357652), (0, 6.1254470328315955), (0, 6.125440806252987), (0, 6.125447112567611), (0, 6.125457850427401), (0, 6.125463595885335), (0, 6.125459635732852), (0, 6.125448186164351), (0, 6.125435846046553), (0, 6.125428678717645), (0, 6.125428506491749), (0, 6.125432548959432), (0, 6.125435984605417), (0, 6.125435292950348), (0, 6.125430200786538), (0, 6.125423302869248), (0, 6.125417961928011), (0, 6.125416034946636), (0, 6.125416889751825), (0, 6.125418194406692), (0, 6.125417735150697), (0, 6.125414891092734), (0, 6.125410793283404), (0, 6.1254072536951005), (0, 6.125405406206664), (0, 6.125405049296677), (0, 6.125405018920352), (0, 6.1254041529889065), (0, 6.125402086796248), (0, 6.125399367421073), (0, 6.125396920335677), (0, 6.125395336837858), (0, 6.125394507439993), (0, 6.1253938121323985), (0, 6.125392650707465), (0, 6.125390876034984), (0, 6.125388825109237), (0, 6.125386987679284), (0, 6.125385617725173), (0, 6.125384586198333), (0, 6.125383543212482), (0, 6.125382216296179), (0, 6.125380600129836), (0, 6.125378914532126), (0, 6.1253773998125665), (0, 6.125376129822478), (0, 6.125374982582103), (0, 6.1253737669105695), (0, 6.125372382492848), (0, 6.1253708826405475), (0, 6.125369405750016), (0, 6.125368051108198), (0, 6.12536680684982), (0, 6.125365578570723), (0, 6.12536427881118), (0, 6.125362895730748), (0, 6.125361489421601), (0, 6.1253601307286285), (0, 6.125358841480261), (0, 6.125357584688535), (0, 6.1253563047982515), (0, 6.12535497665911), (0, 6.125353621028858), (0, 6.125352279091195), (0, 6.125350974167119), (0, 6.125349695030115), (0, 6.125348411298429), (0, 6.1253471024490125), (0, 6.125345773470724), (0, 6.125344446002458), (0, 6.125343136621219), (0, 6.125341843144409), (0, 6.1253405495399305), (0, 6.125339242122049), (0, 6.1253379209662935), (0, 6.125336597261924), (0, 6.125335281283182), (0, 6.125333973244657), (0, 6.125332664714068), (0, 6.125331347532196), (0, 6.125330020949355), (0, 6.1253286908278906), (0, 6.125327362957014), (0, 6.125326037553069), (0, 6.125324709790958), (0, 6.125323374886165), (0, 6.12532203228013), (0, 6.125320685195721), (0, 6.125319336752677), (0, 6.125317986854716), (0, 6.125316632638282), (0, 6.125315271451847), (0, 6.125313903111319), (0, 6.125312529403882), (0, 6.12531115177465), (0, 6.125309769721285), (0, 6.1253083813303215), (0, 6.125306985084273), (0, 6.125305580976535), (0, 6.125304169965281), (0, 6.12530275258106), (0, 6.125301328197356), (0, 6.125299895566971), (0, 6.12529845387264), (0, 6.125297003154089), (0, 6.125295543787067), (0, 6.125294075696527), (0, 6.1252925981446396), (0, 6.125291110216028), (0, 6.1252896113952495), (0, 6.125288101635324), (0, 6.1252865809343655), (0, 6.125285048936976), (0, 6.12528350496379), (0, 6.1252819483609615), (0, 6.1252803787468055), (0, 6.125278795911999), (0, 6.125277199535469), (0, 6.125275589047839), (0, 6.125273963765343), (0, 6.12527232311038), (0, 6.125270666672162), (0, 6.125268994070637), (0, 6.125267304801913), (0, 6.125265598231622), (0, 6.125263873713686), (0, 6.125262130679652), (0, 6.12526036860269), (0, 6.125258586895623), (0, 6.125256784867317), (0, 6.12525496177812), (0, 6.125253116919176), (0, 6.125251249625226), (0, 6.1252493592181585), (0, 6.125247444954581), (0, 6.125245506032027)]