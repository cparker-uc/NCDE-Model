Model Setup for {PATIENT_GROUPS} Trained Network:

NCDE Network Architecture Parameters
Input channels=2
Hidden channels=32
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=100
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=200
Training Results:
Runtime=142.18737411499023
Loss over time=[(0, 38.55589759032963), (0, 30.42788481678721), (0, 18.963789597384547), (0, 20.664452644835063), (0, 16.770238569581622), (0, 17.071680210845706), (0, 16.093108990156605), (0, 15.963859242019138), (0, 15.453249199678442), (0, 15.18032405327264), (0, 14.947105805025927), (0, 14.808198011779352), (0, 14.45613211364571), (0, 14.113850843691786), (0, 14.030851626441889), (0, 13.834957356067413), (0, 13.685387210143466), (0, 13.519342055715146), (0, 13.287453830335737), (0, 13.106961300013115), (0, 12.937789602859779), (0, 12.723805831613543), (0, 12.541090973207481), (0, 12.354085429753178), (0, 12.113731925579096), (0, 12.032594185951039), (0, 11.816548714118845), (0, 11.654128935430935), (0, 11.494039280885923), (0, 11.237531976759515), (0, 11.071000932116904), (0, 10.867356143197606), (0, 10.709994146598497), (0, 10.493115537828487), (0, 10.310762602823967), (0, 10.149206488030654), (0, 10.036423333857211), (0, 9.807935437026995), (0, 9.614256365834814), (0, 9.402261536760944), (0, 9.221992764507611), (0, 9.032325245786563), (0, 8.759075189592673), (0, 8.673123334177118), (0, 8.461635144964475), (0, 8.215207253593093), (0, 8.441811938666065), (0, 7.985620326338991), (0, 7.777802779226013), (0, 8.036121716196417), (0, 7.4418404425892115), (0, 7.49844553681412), (0, 7.370228084637932), (0, 6.955808129517687), (0, 7.149826536548617), (0, 6.993309120259087), (0, 6.465757222687819), (0, 6.985630165828758), (0, 6.826051948688416), (0, 6.281051495783115), (0, 6.824958989282848), (0, 6.348499625668979), (0, 6.586823538342366), (0, 6.63185544400838), (0, 6.7121193940140875), (0, 6.021154539137933), (0, 6.1770383279284), (0, 6.295403128744843), (0, 5.529890642770084), (0, 5.872711758298987), (0, 5.771850405274349), (0, 5.756189883130542), (0, 5.723935017482022), (0, 5.736109409075071), (0, 5.6279149797876595), (0, 5.387093472558269), (0, 5.253248890171387), (0, 5.465755865629894), (0, 5.344365047042824), (0, 5.497024340026389), (0, 5.452683709368699), (0, 4.936391879379091), (0, 5.104571322622648), (0, 5.216103175209134), (0, 5.036968872501607), (0, 4.831378810056375), (0, 5.264242430136279), (0, 4.774485174315882), (0, 5.064547436685023), (0, 5.406576292023017), (0, 4.883328780296105), (0, 4.620372799901594), (0, 4.973766314529529), (0, 4.522308814403216), (0, 4.582248641418135), (0, 4.3859713801631734), (0, 4.380558259530452), (0, 4.4727232069321365), (0, 4.287270750588001), (0, 4.292061163619425)]