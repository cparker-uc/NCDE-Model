Model Setup for {PATIENT_GROUPS} Trained Network:

NCDE Network Architecture Parameters
Input channels=2
Hidden channels=32
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=100
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=200
Training Results:
Runtime=1.3866119384765625
Loss over time=[(0, 25.795057328254007), (0, 31.268830828543866), (0, 29.176494254181204), (0, 24.099173960635042), (0, 24.405955381409743), (0, 25.23984385311196), (0, 24.765064296364734), (0, 25.332676786984113), (0, 23.321068539362646), (0, 22.73318712431367), (0, 21.00072711728279), (0, 23.29231790299897), (0, 23.618738052064217), (0, 22.858598771354682), (0, 21.74492989389398), (0, 20.98478915337242), (0, 19.81106057350595), (0, 20.417568085227323), (0, 16.32144057534847), (0, 15.5449401819961), (0, 15.13195056419362), (0, 15.150455517740136), (0, 15.459399868416126), (0, 15.38886866704403), (0, 14.942301109447055), (0, 14.105230413513944), (0, 14.228837167778762), (0, 13.695661286513348), (0, 14.161665013986193), (0, 14.627645752270546), (0, 13.993769296197534), (0, 14.800025774105874), (0, 19.19020022416714), (0, 14.944851315888235), (0, 12.234227914168917), (0, 13.751017448764294), (0, 13.488170753617174), (0, 13.68551967514668), (0, 13.71487705519673), (0, 13.454990751633945), (0, 12.00044508382677), (0, 10.969102779487732), (0, 13.286888057683173), (0, 11.61568038134712), (0, 10.572927017420405), (0, 11.101794343260854), (0, 11.481246180465748), (0, 11.798789740951298), (0, 14.011117646715302), (0, 14.419533618830387), (0, 16.47432526550555), (0, 16.593136432795745), (0, 15.153543061086266), (0, 14.739368589072434), (0, 15.119158272556184), (0, 14.892546636824619), (0, 14.536739180288384), (0, 14.163410665524445), (0, 13.856428183109518), (0, 13.453563986481965), (0, 13.068495082030495), (0, 12.713594940562261), (0, 12.515138574880712), (0, 12.275309784991855), (0, 11.99361958385752), (0, 11.659421330037391), (0, 11.351710019578958), (0, 11.13878121397201), (0, 11.106135907793792), (0, 10.961711574716098), (0, 10.704722461463566), (0, 10.49437791286166), (0, 10.366009447004773), (0, 10.417870139169697), (0, 10.3753776688859), (0, 10.441721106234017), (0, 10.570207395979587), (0, 10.486053290982037), (0, 10.409476972931703), (0, 10.172999573714554), (0, 10.108553775471002), (0, 9.634783287158843), (0, 10.177765010573417), (0, 10.672587558302517), (0, 10.309187143415171), (0, 9.966835667999474), (0, 7.725442136337453), (0, 7.782111525877371), (0, 8.10389795385164), (0, 7.321791660988137), (0, 4.872022031712488), (0, 5.318141940957395), (0, 4.678554675283731), (0, 4.328312179133024), (0, 4.5557237187223665), (0, 4.324560752530878), (0, 4.049548385928055), (0, 3.7890690855094395), (0, 3.5898706662847046), (0, 3.4680251105665523)]