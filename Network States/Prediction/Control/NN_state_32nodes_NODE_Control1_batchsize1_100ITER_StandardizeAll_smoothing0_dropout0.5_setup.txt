Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=2
Hidden channels=32
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.003
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.003
Weight decay=0.0
Optimizer reset frequency=200

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=StandardizeAll
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=6.4231250286102295
Loss over time=[(0, 6.650871262541134), (0, 6.351768289498124), (0, 6.26284374038236), (0, 6.230354325012717), (0, 6.198480845163274), (0, 6.16719996490393), (0, 6.146964816434055), (0, 6.141726794893617), (0, 6.143983346523999), (0, 6.145978320170408), (0, 6.145453046777618), (0, 6.142755188629514), (0, 6.138720828023891), (0, 6.1342243878936635), (0, 6.130190294130611), (0, 6.127523232420779), (0, 6.126793312821536), (0, 6.127791358667749), (0, 6.12942950259361), (0, 6.130411797965322), (0, 6.130120651715589), (0, 6.12882317936772), (0, 6.127265116973482), (0, 6.126147371175789), (0, 6.1257911041080355), (0, 6.126092295150798), (0, 6.126698656884243), (0, 6.127239510718979), (0, 6.127473934529077), (0, 6.127332133124289), (0, 6.126891382779505), (0, 6.126325007723119), (0, 6.125837778537379), (0, 6.125591390143331), (0, 6.125636012406518), (0, 6.12588340876108), (0, 6.126153109291965), (0, 6.126276398451145), (0, 6.126189714525816), (0, 6.125954164084206), (0, 6.125699086781183), (0, 6.125539115483256), (0, 6.125518236358548), (0, 6.125604210439319), (0, 6.125722184016761), (0, 6.125799378512884), (0, 6.125797176220893), (0, 6.125721113918271), (0, 6.1256108030683345), (0, 6.125517243741564), (0, 6.125477352678526), (0, 6.125496687972459), (0, 6.125549184666396), (0, 6.125594555498066), (0, 6.125602836379136), (0, 6.125570321137749), (0, 6.125517592557739), (0, 6.125473362905382), (0, 6.125456497324271), (0, 6.1254672907393966), (0, 6.125491003290161), (0, 6.125508882762603), (0, 6.125509047972961), (0, 6.125491623922207), (0, 6.125466653357652), (0, 6.1254470328315955), (0, 6.125440806252987), (0, 6.125447112567611), (0, 6.125457850427401), (0, 6.125463595885335), (0, 6.125459635732852), (0, 6.125448186164351), (0, 6.125435846046553), (0, 6.125428678717645), (0, 6.125428506491749), (0, 6.125432548959432), (0, 6.125435984605417), (0, 6.125435292950348), (0, 6.125430200786538), (0, 6.125423302869248), (0, 6.125417961928011), (0, 6.125416034946636), (0, 6.125416889751825), (0, 6.125418194406692), (0, 6.125417735150697), (0, 6.125414891092734), (0, 6.125410793283404), (0, 6.1254072536951005), (0, 6.125405406206664), (0, 6.125405049296677), (0, 6.125405018920352), (0, 6.1254041529889065), (0, 6.125402086796248), (0, 6.125399367421073), (0, 6.125396920335677), (0, 6.125395336837858), (0, 6.125394507439993), (0, 6.1253938121323985), (0, 6.125392650707465), (0, 6.125390876034984)]