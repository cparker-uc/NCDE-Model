Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=4
Hidden channels=32
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=200

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=432.5503685474396
Loss over time=[(0, 16.97981526859039), (0, 17.109353292685697), (0, 16.063092720076945), (0, 15.915446613128402), (0, 16.21886950280036), (0, 16.216078461408205), (0, 16.180434060324487), (0, 16.302243283879545), (0, 16.428328865060504), (0, 16.46209413653457), (0, 16.432042363057107), (0, 16.415634309371914), (0, 16.443624843378633), (0, 16.49943463795911), (0, 16.60574337648255), (0, 16.750901934628878), (0, 16.801044964968487), (0, 16.69826192785294), (0, 16.544975081550213), (0, 16.431786544845995), (0, 16.364683196970198), (0, 16.32745918641819), (0, 16.328407812764496), (0, 16.366951605686285), (0, 16.390295917243243), (0, 16.348927782991314), (0, 16.269597115315804), (0, 16.203222085064425), (0, 16.161910428713206), (0, 16.139275460836032), (0, 16.135856579860963), (0, 16.142863184438188), (0, 16.132532912734867), (0, 16.088938727782605), (0, 16.03004962031234), (0, 15.982657220371662), (0, 15.957369933464243), (0, 15.954455143991062), (0, 15.97320199023216), (0, 16.00457152030382), (0, 16.026950774291358), (0, 16.026286015612378), (0, 16.01230781981132), (0, 16.004482678798443), (0, 16.014610793791636), (0, 16.045393573657122), (0, 16.09081545454036), (0, 16.132010056198204), (0, 16.14778910669804), (0, 16.137730924301177), (0, 16.120452179595077), (0, 16.11053574717122), (0, 16.109962234499946), (0, 16.112900581908214), (0, 16.107961156676645), (0, 16.08368230296331), (0, 16.039293434510128), (0, 15.985347312560084), (0, 15.934238388293716), (0, 15.890127891706843), (0, 15.845072544507248), (0, 15.793765958331633), (0, 15.741740069630136), (0, 15.6909653438922), (0, 15.64048612810352), (0, 15.589770876489231), (0, 15.535581758598557), (0, 15.48663426066338), (0, 15.452795758668522), (0, 15.428522047793908), (0, 15.40032277398455), (0, 15.360027642011389), (0, 15.323646493962107), (0, 15.30471014254634), (0, 15.299671131142354), (0, 15.291022777331118), (0, 15.273394583705597), (0, 15.258145425769396), (0, 15.252285885920411), (0, 15.250704026956509), (0, 15.251745306910877), (0, 15.253587452196552), (0, 15.250660153256607), (0, 15.242578292440687), (0, 15.241579747620769), (0, 15.24811324840126), (0, 15.248391521239194), (0, 15.239800997003233), (0, 15.232477936611247), (0, 15.227811726715357), (0, 15.222529272142879), (0, 15.21733307138254), (0, 15.206739343843896), (0, 15.190777421385182), (0, 15.180707743423566), (0, 15.1741493026421), (0, 15.161499147549346), (0, 15.147308120719394), (0, 15.13474063079214), (0, 15.123004370109262)]