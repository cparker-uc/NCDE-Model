Model Setup for {PATIENT_GROUPS} #{INDIVIDUAL_NUMBER} Trained Network:

NCDE Network Architecture Parameters
Input channels=3
Hidden channels=32
Output channels=3

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=None
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=0
Label smoothing factor=0
Test Patient Combinations:
Training batch size=1
Training Results:
Runtime=182.61932706832886
Loss over time=[(0, 36.640572986988694), (0, 36.99515687589456), (0, 32.774096688795844), (0, 26.24672269057999), (0, 25.57010862550383), (0, 24.473432014863405), (0, 24.492418671509768), (0, 23.649035656909078), (0, 22.629569647019252), (0, 20.414706321683138), (0, 17.960000417105654), (0, 18.592916223603847), (0, 16.84603368970356), (0, 15.850316674947072), (0, 15.027036873038881), (0, 14.531873763971426), (0, 14.054905186245236), (0, 13.69709011007969), (0, 13.350691706395517), (0, 12.984606414831108), (0, 12.591201950464251), (0, 12.353663085833551), (0, 12.117740661555416), (0, 11.885170002415164), (0, 11.74068883754699), (0, 11.564407630743027), (0, 11.383498817469208), (0, 11.232180459118593), (0, 11.050721398997274), (0, 10.849302161965754), (0, 10.734026246640886), (0, 10.55609535516659), (0, 10.29848890242461), (0, 10.117531184283822), (0, 9.860860927379596), (0, 9.612417227775241), (0, 9.303562451283247), (0, 9.102936777333635), (0, 8.987024003491328), (0, 8.862973363886043), (0, 8.677332152307587), (0, 8.557011687112865), (0, 8.270233632289015), (0, 8.173904569263774), (0, 8.459015327706142), (0, 7.8815223796026785), (0, 7.997518586058834), (0, 7.803127002664265), (0, 7.723830581077028), (0, 7.72837161567924), (0, 7.582469728261567), (0, 7.239306565072465), (0, 7.153760072350146), (0, 6.9836903650945885), (0, 6.834360593272275), (0, 6.782611467844859), (0, 6.510392731910667), (0, 6.735526131626589), (0, 6.307040486174162), (0, 6.39144992104336), (0, 6.0517507606938485), (0, 6.170455026909817), (0, 5.69535619104477), (0, 5.783709566684313), (0, 5.480957689803383), (0, 5.5390806999963536), (0, 5.117492363455566), (0, 5.522673004929344), (0, 5.255263406648097), (0, 5.464398624151753), (0, 5.42021742245145), (0, 4.682764814488241), (0, 4.795620806617284), (0, 5.208876834337526), (0, 4.730362957222454), (0, 5.0510467786998765), (0, 4.758755576314107), (0, 4.111517571056365), (0, 4.081056729311878), (0, 3.875316246688058), (0, 4.056102957810515), (0, 3.88801603347488), (0, 7.721719949389301), (0, 6.783706322015539), (0, 5.3512293108152855), (0, 4.359505980206599), (0, 4.090945147613668), (0, 5.388878719784188), (0, 5.154938880256386), (0, 4.414801731619895), (0, 5.059423641143918), (0, 5.541632586626181), (0, 5.435455495431621), (0, 4.678718942656236), (0, 4.699355394939066), (0, 4.974041065655823), (0, 4.94787282961199), (0, 4.801730290002921), (0, 4.578185914072928), (0, 4.5284054871518675)]