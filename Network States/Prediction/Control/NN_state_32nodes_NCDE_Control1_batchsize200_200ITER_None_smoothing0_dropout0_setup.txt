Model Setup for {PATIENT_GROUPS} Trained Network:

NCDE Network Architecture Parameters
Input channels=2
Hidden channels=32
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=200
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=200
Training Results:
Runtime=283.09551787376404
Loss over time=[(0, 38.55589759032963), (0, 30.42788481678721), (0, 18.963789597384547), (0, 20.664452644835063), (0, 16.770238569581622), (0, 17.071680210845706), (0, 16.093108990156605), (0, 15.963859242019138), (0, 15.453249199678442), (0, 15.18032405327264), (0, 14.947105805025927), (0, 14.808198011779352), (0, 14.45613211364571), (0, 14.113850843691786), (0, 14.030851626441889), (0, 13.834957356067413), (0, 13.685387210143466), (0, 13.519342055715146), (0, 13.287453830335737), (0, 13.106961300013115), (0, 12.937789602859779), (0, 12.723805831613543), (0, 12.541090973207481), (0, 12.354085429753178), (0, 12.113731925579096), (0, 12.032594185951039), (0, 11.816548714118845), (0, 11.654128935430935), (0, 11.494039280885923), (0, 11.237531976759515), (0, 11.071000932116904), (0, 10.867356143197606), (0, 10.709994146598497), (0, 10.493115537828487), (0, 10.310762602823967), (0, 10.149206488030654), (0, 10.036423333857211), (0, 9.807935437026995), (0, 9.614256365834814), (0, 9.402261536760944), (0, 9.221992764507611), (0, 9.032325245786563), (0, 8.759075189592673), (0, 8.673123334177118), (0, 8.461635144964475), (0, 8.215207253593093), (0, 8.441811938666065), (0, 7.985620326338991), (0, 7.777802779226013), (0, 8.036121716196417), (0, 7.4418404425892115), (0, 7.49844553681412), (0, 7.370228084637932), (0, 6.955808129517687), (0, 7.149826536548617), (0, 6.993309120259087), (0, 6.465757222687819), (0, 6.985630165828758), (0, 6.826051948688416), (0, 6.281051495783115), (0, 6.824958989282848), (0, 6.348499625668979), (0, 6.586823538342366), (0, 6.63185544400838), (0, 6.7121193940140875), (0, 6.021154539137933), (0, 6.1770383279284), (0, 6.295403128744843), (0, 5.529890642770084), (0, 5.872711758298987), (0, 5.771850405274349), (0, 5.756189883130542), (0, 5.723935017482022), (0, 5.736109409075071), (0, 5.6279149797876595), (0, 5.387093472558269), (0, 5.253248890171387), (0, 5.465755865629894), (0, 5.344365047042824), (0, 5.497024340026389), (0, 5.452683709368699), (0, 4.936391879379091), (0, 5.104571322622648), (0, 5.216103175209134), (0, 5.036968872501607), (0, 4.831378810056375), (0, 5.264242430136279), (0, 4.774485174315882), (0, 5.064547436685023), (0, 5.406576292023017), (0, 4.883328780296105), (0, 4.620372799901594), (0, 4.973766314529529), (0, 4.522308814403216), (0, 4.582248641418135), (0, 4.3859713801631734), (0, 4.380558259530452), (0, 4.4727232069321365), (0, 4.287270750588001), (0, 4.292061163619425), (0, 4.306110739727594), (0, 4.293444123261785), (0, 4.1129303880413985), (0, 3.969654025358375), (0, 4.005710801090041), (0, 3.9389479172309696), (0, 3.8965159601957207), (0, 4.0430842012997985), (0, 3.8766034853409663), (0, 4.22039190915567), (0, 3.8691586577832293), (0, 4.189394486830032), (0, 3.743644278843159), (0, 4.04234427695764), (0, 3.786152791290911), (0, 3.9008407679846853), (0, 3.700412388476323), (0, 3.7836274654843858), (0, 3.6755700484830247), (0, 3.6611248137336077), (0, 3.6536844560848944), (0, 3.7082207852493343), (0, 3.6389996958663176), (0, 3.7244106022406935), (0, 3.659775679023626), (0, 3.7048135220093186), (0, 3.5937087630902727), (0, 3.8136784001468143), (0, 3.5925929488967965), (0, 3.868868609801845), (0, 3.5326048827247463), (0, 3.6810277821994792), (0, 3.5490683853098535), (0, 3.578478338742183), (0, 3.483639166976637), (0, 3.7505660394583065), (0, 3.4875791955646425), (0, 3.484970341703266), (0, 3.5473064950682414), (0, 3.4451848176213917), (0, 3.4553703184645665), (0, 3.4085346270878376), (0, 3.45869579934407), (0, 3.5048261611427836), (0, 3.548175002918789), (0, 3.370940129226372), (0, 3.599610661399325), (0, 3.391845987446585), (0, 3.5552093250516674), (0, 3.5126572043437054), (0, 3.5149735770597474), (0, 3.3857612821100833), (0, 3.394858828081506), (0, 3.293099261803528), (0, 3.26174573051055), (0, 3.3754815418814075), (0, 3.354406157912841), (0, 3.290335048765158), (0, 3.234272231477115), (0, 3.229868668506796), (0, 3.2208853661081758), (0, 3.1930949501532813), (0, 3.7092541431562256), (0, 3.876147757720275), (0, 3.705132761725859), (0, 3.8503967465123807), (0, 4.676403054598751), (0, 3.629885579613142), (0, 4.162477864847092), (0, 4.2115611869957155), (0, 3.708534858121598), (0, 4.159058065636067), (0, 3.809387283449412), (0, 3.872572220616985), (0, 4.067273046871935), (0, 3.6294369282380483), (0, 4.0107373898895196), (0, 3.687372016827024), (0, 3.7050837760942246), (0, 3.8662039603023044), (0, 3.494915198781962), (0, 3.710784720012636), (0, 3.470466092874211), (0, 3.571261791814427), (0, 3.550532211520805), (0, 3.3901441829440695), (0, 3.6235491574971785), (0, 3.3540048068463495), (0, 3.6359868926569594), (0, 3.57310819182498), (0, 3.2979148835421204), (0, 3.5019344691766325), (0, 3.2907583619615877), (0, 3.3845105643608395), (0, 3.3828524200788674), (0, 3.335674968235422), (0, 3.3559264270768336), (0, 3.2903899516148467), (0, 3.246061162036155), (0, 3.2798303794764703)]