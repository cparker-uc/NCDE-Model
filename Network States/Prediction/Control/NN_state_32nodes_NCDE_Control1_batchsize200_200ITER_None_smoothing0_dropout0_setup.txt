Model Setup for {PATIENT_GROUPS} Trained Network:

NCDE Network Architecture Parameters
Input channels=2
Hidden channels=32
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=200
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=200
Training Results:
Runtime=2.815429925918579
Loss over time=[(0, 25.795057328254007), (0, 31.268830828543866), (0, 29.176494254181204), (0, 24.099173960635042), (0, 24.405955381409743), (0, 25.23984385311196), (0, 24.765064296364734), (0, 25.332676786984113), (0, 23.321068539362646), (0, 22.73318712431367), (0, 21.00072711728279), (0, 23.29231790299897), (0, 23.618738052064217), (0, 22.858598771354682), (0, 21.74492989389398), (0, 20.98478915337242), (0, 19.81106057350595), (0, 20.417568085227323), (0, 16.32144057534847), (0, 15.5449401819961), (0, 15.13195056419362), (0, 15.150455517740136), (0, 15.459399868416126), (0, 15.38886866704403), (0, 14.942301109447055), (0, 14.105230413513944), (0, 14.228837167778762), (0, 13.695661286513348), (0, 14.161665013986193), (0, 14.627645752270546), (0, 13.993769296197534), (0, 14.800025774105874), (0, 19.19020022416714), (0, 14.944851315888235), (0, 12.234227914168917), (0, 13.751017448764294), (0, 13.488170753617174), (0, 13.68551967514668), (0, 13.71487705519673), (0, 13.454990751633945), (0, 12.00044508382677), (0, 10.969102779487732), (0, 13.286888057683173), (0, 11.61568038134712), (0, 10.572927017420405), (0, 11.101794343260854), (0, 11.481246180465748), (0, 11.798789740951298), (0, 14.011117646715302), (0, 14.419533618830387), (0, 16.47432526550555), (0, 16.593136432795745), (0, 15.153543061086266), (0, 14.739368589072434), (0, 15.119158272556184), (0, 14.892546636824619), (0, 14.536739180288384), (0, 14.163410665524445), (0, 13.856428183109518), (0, 13.453563986481965), (0, 13.068495082030495), (0, 12.713594940562261), (0, 12.515138574880712), (0, 12.275309784991855), (0, 11.99361958385752), (0, 11.659421330037391), (0, 11.351710019578958), (0, 11.13878121397201), (0, 11.106135907793792), (0, 10.961711574716098), (0, 10.704722461463566), (0, 10.49437791286166), (0, 10.366009447004773), (0, 10.417870139169697), (0, 10.3753776688859), (0, 10.441721106234017), (0, 10.570207395979587), (0, 10.486053290982037), (0, 10.409476972931703), (0, 10.172999573714554), (0, 10.108553775471002), (0, 9.634783287158843), (0, 10.177765010573417), (0, 10.672587558302517), (0, 10.309187143415171), (0, 9.966835667999474), (0, 7.725442136337453), (0, 7.782111525877371), (0, 8.10389795385164), (0, 7.321791660988137), (0, 4.872022031712488), (0, 5.318141940957395), (0, 4.678554675283731), (0, 4.328312179133024), (0, 4.5557237187223665), (0, 4.324560752530878), (0, 4.049548385928055), (0, 3.7890690855094395), (0, 3.5898706662847046), (0, 3.4680251105665523), (0, 3.363279885846242), (0, 3.2566402954332436), (0, 3.1162389296732753), (0, 2.908760049608796), (0, 2.7684729098778735), (0, 2.4894033591811846), (0, 3.1877057682439314), (0, 2.7066354947005107), (0, 2.5161331033266516), (0, 2.424252542099896), (0, 2.3329991329967394), (0, 2.140959273401759), (0, 3.240991896093949), (0, 4.985170804093884), (0, 4.904686291428309), (0, 4.859878796421722), (0, 4.658189661110502), (0, 4.611486941081599), (0, 2.892345726180204), (0, 1.8975077398503026), (0, 2.2338023566404592), (0, 2.5980886432699575), (0, 2.896125782403912), (0, 3.7909681227773673), (0, 4.831814353774701), (0, 4.785064113602071), (0, 4.8914260068125825), (0, 6.102887455778895), (0, 4.5132042265872645), (0, 4.582866824435818), (0, 5.688722146030035), (0, 2.862189378901168), (0, 2.914933644120968), (0, 2.741088420634494), (0, 2.2206511443067694), (0, 3.9528645120587282), (0, 3.597042217584637), (0, 5.789423671203272), (0, 4.4635983371557435), (0, 4.624893930418162), (0, 4.503874709386312), (0, 2.962228150081215), (0, 3.130566605726288), (0, 2.4139121893646904), (0, 3.8524495909667293), (0, 3.3796464850581485), (0, 2.8723316347113217), (0, 2.9566783606049656), (0, 3.034873221134111), (0, 3.2755603790722643), (0, 3.070634210411033), (0, 3.07543450734458), (0, 3.060208507342115), (0, 2.7686258600362468), (0, 2.671725108556059), (0, 3.0486378820336726), (0, 2.6453765156171616), (0, 2.3584489180948367), (0, 2.214103713564505), (0, 2.139381940098336), (0, 2.066144937382266), (0, 1.9884088870421792), (0, 1.901980646704247), (0, 1.857744870485966), (0, 1.8284339630135507), (0, 1.8184114249829568), (0, 1.8212355441562271), (0, 1.833699887169342), (0, 1.946922589102982), (0, 1.8493911834833165), (0, 1.697558367649844), (0, 1.5733242874276252), (0, 1.473601496844086), (0, 1.3882393060873475), (0, 1.325615798142199), (0, 1.2586032285927757), (0, 1.1972986827744376), (0, 1.1643866848526219), (0, 1.1415823737263382), (0, 1.140069682710701), (0, 1.1211878035159466), (0, 1.0865078175408327), (0, 1.036945546024963), (0, 0.9855460527325692), (0, 0.9335352134916025), (0, 0.8906769158732047), (0, 0.9149453655705192), (0, 0.9057688729341755), (0, 0.861587726446219), (0, 0.8123865251934942), (0, 0.7852210790901363), (0, 0.7532283384291052), (0, 0.7456079845412336), (0, 0.7402618849069136), (0, 0.7276217290386271), (0, 0.7144988542564009), (0, 0.7232501440418454), (0, 0.7244087067097627), (0, 0.7196170325358222), (0, 0.7506619916071586)]