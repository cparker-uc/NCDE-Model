Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=2
Hidden channels=128
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 1e-06
)Training Iterations=200
Learning rate=0.001
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=StandardizeAll
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=4646.846442222595
Loss over time=[(0, 4.05501513258226), (0, 4.039973519313417), (0, 4.027449710688372), (0, 4.0175424121573755), (0, 4.010143115490215), (0, 4.004362946092481), (0, 4.000248265954697), (0, 3.997788520595591), (0, 3.996630777638541), (0, 3.996519356122987), (0, 3.9971278873403193), (0, 3.9980879573096377), (0, 3.9990461827752695), (0, 3.9997310673988924), (0, 3.9999918263953393), (0, 3.9998142698206083), (0, 3.9992753109887222), (0, 3.998504590020279), (0, 3.9976369987142717), (0, 3.996793948740043), (0, 3.996065513325148), (0, 3.9955083246381133), (0, 3.995141448495255), (0, 3.994953253621976), (0, 3.9949154758134555), (0, 3.9949762443082664), (0, 3.995076202415481), (0, 3.9951651099363588), (0, 3.995204950140082), (0, 3.9951743144294274), (0, 3.9950665508090952), (0, 3.994887980551278), (0, 3.9946554301892845), (0, 3.9943892724102947), (0, 3.9941134148724804), (0, 3.993847470796011), (0, 3.9936083538798286), (0, 3.993403555905934), (0, 3.9932360405310905), (0, 3.9931021772388293), (0, 3.992992924075149), (0, 3.9928980135813306), (0, 3.9928060927782174), (0, 3.99270584048667), (0, 3.9925924309162215), (0, 3.992465431080802), (0, 3.992325050105263), (0, 3.9921744102770136), (0, 3.992018771527562), (0, 3.991863695295531), (0, 3.9917148199974046), (0, 3.9915731419966307), (0, 3.9914390952711885), (0, 3.991311780439423), (0, 3.991187969663461), (0, 3.991063627673914), (0, 3.990936372718963), (0, 3.9908037243849894), (0, 3.9906633366066173), (0, 3.9905167238480646), (0, 3.9903652201068933), (0, 3.9902107691539355), (0, 3.9900556362823263), (0, 3.989899960926726), (0, 3.989746290552736), (0, 3.9895942540218017), (0, 3.9894436609992736), (0, 3.989292568686065), (0, 3.9891399611842218), (0, 3.988984570120674), (0, 3.988825769367669), (0, 3.988664552273927), (0, 3.9885016592403044), (0, 3.988338126092215), (0, 3.9881754047977025), (0, 3.988012784944044), (0, 3.9878511488237165), (0, 3.9876902511733037), (0, 3.98752862086675), (0, 3.9873668529712574), (0, 3.9872042352467676), (0, 3.9870406584406592), (0, 3.9868767200738087), (0, 3.98671262678708), (0, 3.986548913826444), (0, 3.9863860553361152), (0, 3.986224152990555), (0, 3.986063456681454), (0, 3.985903823048247), (0, 3.985745117660968), (0, 3.9855875254485724), (0, 3.9854310163311353), (0, 3.9852757274128265), (0, 3.985121926150357), (0, 3.9849699267097467), (0, 3.984819706098942), (0, 3.9846716797244444), (0, 3.9845259329871756), (0, 3.9843823258541833), (0, 3.984240804181356), (0, 3.9841013575076643), (0, 3.98396428334278), (0, 3.983829467966562), (0, 3.9836971184265875), (0, 3.983567359530134), (0, 3.983440137453069), (0, 3.9833156460981525), (0, 3.9831939335474598), (0, 3.98307491891935), (0, 3.9829586946728144), (0, 3.982845141293031), (0, 3.9827345568406254), (0, 3.9826267312051873), (0, 3.9825214975053598), (0, 3.982419089671606), (0, 3.982319248366922), (0, 3.9822222957278943), (0, 3.9821278485386387), (0, 3.9820359402495527), (0, 3.9819465985471187), (0, 3.981859556520209), (0, 3.981774841909471), (0, 3.9816922339332903), (0, 3.981612093061605), (0, 3.9815338803311424), (0, 3.981457907938004), (0, 3.981383664476615), (0, 3.9813113827705338), (0, 3.981240793361131), (0, 3.981171809415315), (0, 3.981104413984625), (0, 3.981038173217091), (0, 3.980973435492952), (0, 3.9809100786573257), (0, 3.980848518782136), (0, 3.9807884455170295), (0, 3.980729423477159), (0, 3.9806720096668684), (0, 3.980615453004555), (0, 3.980559990988279), (0, 3.9805054464597123), (0, 3.980450751806288), (0, 3.980396671588999), (0, 3.980342880706564), (0, 3.980290055673059), (0, 3.9802380087426896), (0, 3.9801866448935046), (0, 3.980134866041539), (0, 3.9800842781284604), (0, 3.980033686229635), (0, 3.9799829921061853), (0, 3.9799326180226258), (0, 3.9798826264468006), (0, 3.9798332303891004), (0, 3.979784735033557), (0, 3.979737649415524), (0, 3.9796927602348076), (0, 3.979649482101381), (0, 3.9796075850140893), (0, 3.9795668201291265), (0, 3.979528475710535), (0, 3.979491556344323), (0, 3.979456511303287), (0, 3.9794214951426725), (0, 3.97938734551322), (0, 3.9793520662627415), (0, 3.9793171291379377), (0, 3.979281707788812), (0, 3.9792469461144524), (0, 3.9792122846099773), (0, 3.979178904732808), (0, 3.979146591757542), (0, 3.979115554554578), (0, 3.9790865742411383), (0, 3.9790587134480764), (0, 3.9790318241238043), (0, 3.979005344842744), (0, 3.978979037549547), (0, 3.9789524346344156), (0, 3.97892574654212), (0, 3.9789010518136037), (0, 3.978872426274764), (0, 3.9788464372317094), (0, 3.9788183308352334), (0, 3.978796517623518), (0, 3.978772609117168), (0, 3.978749783283466), (0, 3.9787273988640504), (0, 3.9787053900334444), (0, 3.978684332372632), (0, 3.978663479780209), (0, 3.978643015606809), (0, 3.978622483177766), (0, 3.9786027032116436), (0, 3.97858263807772), (0, 3.9785626303612633), (0, 3.9785425626002437), (0, 3.978522527091485), (0, 3.9785026565513864), (0, 3.9784827563565726)]