Model Setup for {PATIENT_GROUPS} #{INDIVIDUAL_NUMBER} Trained Network:

NCDE Network Architecture Parameters
Input channels=3
Hidden channels=16
Output channels=3

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=None
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=0
Label smoothing factor=0
Test Patient Combinations:
Training batch size=1
Training Results:
Runtime=111.2748818397522
Loss over time=[(0, 44.0470771410177), (0, 40.898777761668946), (0, 38.00846870369415), (0, 35.91902252285894), (0, 33.07982809088596), (0, 31.78925600246247), (0, 30.993593073268116), (0, 29.90850789346236), (0, 28.841769573768826), (0, 27.960490063089168), (0, 27.154985214926835), (0, 26.309648822009446), (0, 25.587326995695182), (0, 25.084982524478978), (0, 24.564869674636554), (0, 24.076811703635883), (0, 23.485963703559843), (0, 22.680854861807802), (0, 21.732818677038935), (0, 21.68270597414018), (0, 20.798978282569262), (0, 19.458538235382804), (0, 18.566810306422372), (0, 17.853378434094246), (0, 17.417253404516014), (0, 16.934490536906356), (0, 16.18128966019356), (0, 15.238659936137703), (0, 14.534440928291147), (0, 13.847525478855436), (0, 13.524444403720393), (0, 13.48197351909937), (0, 13.604254622090494), (0, 14.062994145985295), (0, 14.669034877608835), (0, 15.109326751822001), (0, 14.779579797857584), (0, 14.263024900421186), (0, 13.925560085863667), (0, 13.182646718310911), (0, 12.631019105779544), (0, 12.388026885654101), (0, 12.421318337216974), (0, 12.444664219509287), (0, 12.617885617554087), (0, 11.980671747331845), (0, 11.609215846620911), (0, 11.389366030059819), (0, 11.465861438446412), (0, 11.13111595398906), (0, 11.264271709455222), (0, 11.383445470485754), (0, 11.264687090733903), (0, 11.123006831342407), (0, 11.732867333548384), (0, 11.459175158001102), (0, 11.687668255370026), (0, 11.529931621751352), (0, 11.027270986438955), (0, 10.240092217308584), (0, 10.298666420397339), (0, 9.859532779139403), (0, 9.42028878926984), (0, 9.448046001918355), (0, 9.616409824418318), (0, 9.625554291714145), (0, 9.893868932993945), (0, 9.684942896397922), (0, 9.477738408349502), (0, 9.494276407449787), (0, 9.5619985325088), (0, 9.84959281020217), (0, 9.316562024000186), (0, 8.67827653965733), (0, 8.160956194011087), (0, 8.280800463895861), (0, 7.8231821769816925), (0, 7.972390600700816), (0, 8.013460111652302), (0, 7.883012030779959), (0, 7.580561912908824), (0, 7.283276664358207), (0, 7.3296216379295345), (0, 7.8352005143547325), (0, 7.586554025629624), (0, 7.948799631053329), (0, 7.874273040862579), (0, 8.569825993025091), (0, 7.8734194108203), (0, 7.101401298524706), (0, 6.852007773489383), (0, 6.839831252284604), (0, 6.602464429496653), (0, 7.365158179202493), (0, 7.617711360631234), (0, 7.532195247437329), (0, 7.300579479871189), (0, 7.07343985828997), (0, 7.094000543195818), (0, 6.871887650750058)]