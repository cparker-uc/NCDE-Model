Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=2
Hidden channels=512
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.003
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.003
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=StandardizeAll
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=191.20739674568176
Loss over time=[(0, 6.434693277869852), (0, 6.150124926305481), (0, 6.133066303454546), (0, 6.127332228823221), (0, 6.126565837293895), (0, 6.12955771260439), (0, 6.126348216454368), (0, 6.127067037439218), (0, 6.126348478042889), (0, 6.125622901647376), (0, 6.126344613547178), (0, 6.125318774574844), (0, 6.126088214958239), (0, 6.124834995222505), (0, 6.1256521175175855), (0, 6.124719294705472), (0, 6.125345882551303), (0, 6.124462890229719), (0, 6.124967081218187), (0, 6.12433944106369), (0, 6.124664663673977), (0, 6.1241293119387), (0, 6.124353574278283), (0, 6.12398650299726), (0, 6.124067272458074), (0, 6.123789262082894), (0, 6.123802813458346), (0, 6.123625343273161), (0, 6.12353993764199), (0, 6.12343116426219), (0, 6.123307242954639), (0, 6.123248668060773), (0, 6.123072828177547), (0, 6.12305254305646), (0, 6.122864145137518), (0, 6.122857485427574), (0, 6.122660375766198), (0, 6.122657683854976), (0, 6.12247298540598), (0, 6.12245893266365), (0, 6.122297134044323), (0, 6.122260273002922), (0, 6.122128604993225), (0, 6.12207017330764), (0, 6.121971268443769), (0, 6.121886579282298), (0, 6.121815101056233), (0, 6.121719123016145), (0, 6.121665367357426), (0, 6.121565565176066), (0, 6.121516113290799), (0, 6.121427377284494), (0, 6.121374498383932), (0, 6.1213028294364795), (0, 6.121240333881112), (0, 6.121185491176269), (0, 6.121120288810133), (0, 6.121075818479429), (0, 6.121014034969904), (0, 6.120970713872432), (0, 6.120919723278672), (0, 6.120874943507283), (0, 6.120834893831392), (0, 6.120789363146901), (0, 6.120755276212859), (0, 6.120714907304758), (0, 6.1206823725363115), (0, 6.120649479693889), (0, 6.120616579435296), (0, 6.120589521108219), (0, 6.120559298907432), (0, 6.120534757386095), (0, 6.120509237733401), (0, 6.120485043560226), (0, 6.120463902703812), (0, 6.120441379067108), (0, 6.120422508476108), (0, 6.120402964593509), (0, 6.120384701273909), (0, 6.12036815259215), (0, 6.120351030460554), (0, 6.12033619081333), (0, 6.120320969306655), (0, 6.120306718437652), (0, 6.1202934613418885), (0, 6.120279987855881), (0, 6.12026792838206), (0, 6.12025564588094), (0, 6.120244065055644), (0, 6.12023303707287), (0, 6.120221962287754), (0, 6.120211751480249), (0, 6.120201391015687), (0, 6.120191544605637), (0, 6.1201819629238265), (0, 6.120172428150172), (0, 6.120163397413563), (0, 6.1201542601368), (0, 6.120145502861683), (0, 6.120136824780104), (0, 6.120128253590906), (0, 6.120119936624642), (0, 6.120111562120247), (0, 6.120103450759758), (0, 6.12009531852658), (0, 6.120087324138754), (0, 6.120079407559459), (0, 6.1200714977350374), (0, 6.120063727954527), (0, 6.120055919334977), (0, 6.120048232028183), (0, 6.12004052309014), (0, 6.1200328737310405), (0, 6.120025247550413), (0, 6.120017625155242), (0, 6.120010051530491), (0, 6.120002449751868), (0, 6.119994898545304), (0, 6.119987315960507), (0, 6.119979767575277), (0, 6.119972195426896), (0, 6.119964635640444), (0, 6.119957063876893), (0, 6.119949485104951), (0, 6.119941902521895), (0, 6.119934301699749), (0, 6.119926698540298), (0, 6.119919070834451), (0, 6.119911439652756), (0, 6.119903780522777), (0, 6.119896115323752), (0, 6.119888421418576), (0, 6.119880718149242), (0, 6.119872985751493), (0, 6.119865240973318), (0, 6.119857467410482), (0, 6.11984967880185), (0, 6.119841861556321), (0, 6.119834027026485), (0, 6.119826164487564), (0, 6.119818282606146), (0, 6.119810373396547), (0, 6.119802442922665), (0, 6.119794486359338), (0, 6.119786506560133), (0, 6.119778502020861), (0, 6.119770472557992), (0, 6.119762419797286), (0, 6.119754340891519), (0, 6.119746239476439), (0, 6.119738111932483), (0, 6.119729961714972), (0, 6.119721786504818), (0, 6.1197135876806925), (0, 6.119705365736575), (0, 6.119697119431993), (0, 6.119688851063443), (0, 6.119680559190622), (0, 6.119672245031008), (0, 6.119663909130173), (0, 6.119655550897755), (0, 6.119647171907373), (0, 6.119638771926461), (0, 6.119630351280711), (0, 6.119621911246825), (0, 6.119613451670286), (0, 6.119604973162634), (0, 6.1195964768261355), (0, 6.1195879628311625), (0, 6.119579431773567), (0, 6.119570884684155), (0, 6.11956232218239), (0, 6.119553744820674), (0, 6.119545153411753), (0, 6.1195365490114115), (0, 6.119527932388437), (0, 6.119519304171863), (0, 6.119510665427938), (0, 6.119502017174802), (0, 6.11949336035556), (0, 6.119484696000097), (0, 6.119476025019477), (0, 6.119467348628663), (0, 6.11945866776151), (0, 6.11944998384858), (0, 6.119441297901588), (0, 6.119432611909033), (0, 6.119423927776117), (0, 6.119415252308181), (0, 6.119406603396269), (0, 6.119398066477615), (0, 6.119390027608531), (0, 6.119384579432324), (0, 6.119393524798252), (0, 6.119488787313599), (0, 6.120051555826819), (0, 6.12268439627021), (0, 6.124706140897765), (0, 6.126585649045048), (0, 6.1194598206931525)]