Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=2
Hidden channels=128
Output channels=2

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.006
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.006
Weight decay=0.0
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=StandardizeAll
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=17.06072688102722
Loss over time=[(0, 1.309719229630257), (0, 1.1721436614863772), (0, 1.0429861174788266), (0, 0.9359823304079335), (0, 0.8566717414946137), (0, 0.8021734017552246), (0, 0.7658780172624585), (0, 0.7410002929068042), (0, 0.7228498331265133), (0, 0.7087273146939315), (0, 0.6970680510944416), (0, 0.6869458064449151), (0, 0.6779058815786723), (0, 0.6701263367853104), (0, 0.6651776758948225), (0, 0.6645487912089758), (0, 0.6601588738526768), (0, 0.654880887330659), (0, 0.6518677820405986), (0, 0.6504553518295159), (0, 0.6494148191280148), (0, 0.6481262003411804), (0, 0.646413201618105), (0, 0.6443457826380967), (0, 0.6422175119404483), (0, 0.640608877721606), (0, 0.640129958747539), (0, 0.6400346045426529), (0, 0.6387176776567727), (0, 0.6371407530229226), (0, 0.6363696903904846), (0, 0.6361049053517004), (0, 0.6357945181383431), (0, 0.6351838813934393), (0, 0.6343147952512426), (0, 0.6334542209645737), (0, 0.6329588916585125), (0, 0.6328497608212457), (0, 0.6325301650448361), (0, 0.6318223969387349), (0, 0.6312666655208915), (0, 0.6310271235816773), (0, 0.6308523578990084), (0, 0.6305342059415238), (0, 0.6300740422565206), (0, 0.6296413366981832), (0, 0.6294015008772083), (0, 0.6292616867660127), (0, 0.6289692421194433), (0, 0.6285853861436368), (0, 0.6283230939406335), (0, 0.6281647025598557), (0, 0.627973229634496), (0, 0.6276990043656597), (0, 0.6274167521142733), (0, 0.6272201440970228), (0, 0.6270749377923416), (0, 0.6268671447911195), (0, 0.626619817230356), (0, 0.6264286500032195), (0, 0.6262821105433306), (0, 0.6261120909666343), (0, 0.6259066872912075), (0, 0.6257165171479145), (0, 0.6255700474767123), (0, 0.6254225215706265), (0, 0.6252425135418531), (0, 0.6250704741584182), (0, 0.6249296189770908), (0, 0.6247900856975441), (0, 0.6246313659200591), (0, 0.6244736195045424), (0, 0.6243378328269961), (0, 0.6242067130555282), (0, 0.6240609113645351), (0, 0.62391747115399), (0, 0.6237899494021724), (0, 0.6236638545471913), (0, 0.6235290142132545), (0, 0.6233969625339681), (0, 0.6232762467231453), (0, 0.6231557457958363), (0, 0.6230293585047191), (0, 0.6229076811096873), (0, 0.6227934125637398), (0, 0.6226776179893165), (0, 0.6225593187397803), (0, 0.6224459588285988), (0, 0.6223370680703474), (0, 0.6222261358717862), (0, 0.6221152413382641), (0, 0.6220091082565021), (0, 0.6219046451988), (0, 0.6217988014193429), (0, 0.6216948136404504), (0, 0.6215943158553209), (0, 0.6214939551931685), (0, 0.6213934471942791), (0, 0.6212956366434259), (0, 0.6211996287253023), (0, 0.6211033891689023), (0, 0.6210083294769801), (0, 0.6209155703719481), (0, 0.6208234103490551), (0, 0.620731593035922), (0, 0.6206416027103412), (0, 0.6205529181327956), (0, 0.6204645209036636), (0, 0.6203772601962081), (0, 0.6202915404444874), (0, 0.620206403676383), (0, 0.6201219687687818), (0, 0.6200389241756133), (0, 0.6199567593465748), (0, 0.619875148203789), (0, 0.6197946521666337), (0, 0.6197151884953336), (0, 0.6196362998517224), (0, 0.619558295427208), (0, 0.6194813265953593), (0, 0.6194050113219277), (0, 0.6193294298426085), (0, 0.6192548137936593), (0, 0.6191809130942325), (0, 0.6191076706727044), (0, 0.6190353006592706), (0, 0.6189636711784186), (0, 0.6188926679598888), (0, 0.618822450343094), (0, 0.6187529693960717), (0, 0.6186841003234171), (0, 0.6186159485120509), (0, 0.6185485091542376), (0, 0.6184816721379583), (0, 0.6184154995315534), (0, 0.6183500073061124), (0, 0.6182851056301031), (0, 0.6182208285015013), (0, 0.6181571969091834), (0, 0.618094140943345), (0, 0.618031677472207), (0, 0.6179698259759653), (0, 0.6179085324461734), (0, 0.6178478046751388), (0, 0.617787657486638), (0, 0.6177280489821823), (0, 0.6176689824948609), (0, 0.6176104678171821), (0, 0.6175524719579483), (0, 0.6174949964565323), (0, 0.6174380463026096), (0, 0.6173815951846149), (0, 0.6173256440548597), (0, 0.6172701939033964), (0, 0.61721522359422), (0, 0.6171607339639752), (0, 0.6171067226943147), (0, 0.6170531728767636), (0, 0.6170000851929216), (0, 0.6169474548467003), (0, 0.61689526849874), (0, 0.6168435264481592), (0, 0.6167922221211355), (0, 0.616741345253627), (0, 0.6166908954338611), (0, 0.6166408650858262), (0, 0.616591246526865), (0, 0.6165420383545066), (0, 0.6164932326676467), (0, 0.6164448238316872), (0, 0.6163968093085254), (0, 0.6163491815124205), (0, 0.6163019362489471), (0, 0.6162550698723722), (0, 0.6162085755755037), (0, 0.6161624499773599), (0, 0.6161166885720716), (0, 0.6160712855912794), (0, 0.616026237898429), (0, 0.6159815405109579), (0, 0.6159371887041538), (0, 0.6158931791784105), (0, 0.6158495069270011), (0, 0.615806168052263), (0, 0.6157631589095922), (0, 0.6157204748388265), (0, 0.6156781124252042), (0, 0.6156360677408012), (0, 0.6155943366719545), (0, 0.6155529159462988), (0, 0.6155118015682581), (0, 0.6154709899538273), (0, 0.6154304777671104), (0, 0.6153902611957648), (0, 0.6153503370121324), (0, 0.6153107017903624), (0, 0.6152713520522803), (0, 0.6152322847159823), (0, 0.6151934963842233), (0, 0.6151549839123907)]