Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=1
Hidden channels=256
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.003
    maximize: False
    weight_decay: 1e-06
)Training Iterations=100
Learning rate=0.003
Weight decay=1e-06
Optimizer reset frequency=None

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=311.6505491733551
Loss over time=[(0, 21.175729840168543), (0, 8.403510708026012), (0, 4.773616559537908), (0, 5.55917239672831), (0, 3.671296403266871), (0, 7.068054610300701), (0, 6.602430248721848), (0, 6.81893135367181), (0, 5.5652015594174795), (0, 3.911831959445185), (0, 3.176150923907458), (0, 3.021562024936431), (0, 2.4071187474603915), (0, 2.4022849984019223), (0, 2.4462319368698684), (0, 2.6684460149236395), (0, 2.5443300959643014), (0, 2.473322493330957), (0, 2.281684403089942), (0, 2.279076608933594), (0, 2.2970598381554996), (0, 2.2295684421822144), (0, 2.29861605342793), (0, 2.1985809178400175), (0, 2.1783050572646285), (0, 2.187723464641541), (0, 2.137962716130639), (0, 2.1585719265567818), (0, 2.1362295464344183), (0, 2.132694739932727), (0, 2.116577558437998), (0, 2.109942938801101), (0, 2.116010161281129), (0, 2.100757492999844), (0, 2.0884108373514896), (0, 2.0756638890996384), (0, 2.0738437246381793), (0, 2.0513164029089865), (0, 2.0402983172422586), (0, 2.047106287848639), (0, 2.112169935248356), (0, 2.0454039784635762), (0, 2.060163901176719), (0, 2.0035447742782284), (0, 2.0457883060038946), (0, 1.9906506530737471), (0, 2.0229682494225867), (0, 1.9888445365599061), (0, 1.9994758464169564), (0, 1.9774869421797638), (0, 1.987281021033534), (0, 1.9494669280064834), (0, 1.9482653018002962), (0, 1.9295645146537275), (0, 1.9159665142914382), (0, 1.9021089214066027), (0, 1.8813301658234147), (0, 1.8679583492362462), (0, 1.8560178616195067), (0, 1.867804355519731), (0, 1.846931796121124), (0, 1.8732787964953392), (0, 1.7898090540831122), (0, 2.394565499249381), (0, 3.306140987196907), (0, 3.094071039503056), (0, 2.362128906760835), (0, 2.372009334605008), (0, 2.1622868103985686), (0, 2.227720826684561), (0, 2.2284948224474332), (0, 2.135086533904543), (0, 2.104911657371897), (0, 2.012323925606074), (0, 1.9946156372593158), (0, 2.0186839681147397), (0, 2.0176380527020474), (0, 1.9914826490516697), (0, 1.9695069303129193), (0, 1.9634341403032103), (0, 1.9712675519598615), (0, 1.971340963387841), (0, 1.9687411017898204), (0, 1.96325047206528), (0, 1.9515968523925145), (0, 1.9365398496258295), (0, 1.9267495466712092), (0, 1.9239365469764502), (0, 1.9176898652567131), (0, 1.9100954211568275), (0, 1.9043378093224812), (0, 1.9016051713815154), (0, 1.8978986958684416), (0, 1.8933845551983148), (0, 1.8872205250884961), (0, 1.8783718649524435), (0, 1.8730485283924754), (0, 1.870299414264636), (0, 1.8606508569266924), (0, 1.8539124989561309)]