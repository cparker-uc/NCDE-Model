Model Setup for {PATIENT_GROUPS} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=512
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 1e-06
)Training Iterations=100
Learning rate=0.0003
Weight decay=1e-06
Optimizer reset frequency=500

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=1
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=71.64103984832764
Loss over time=[(0, 554.8195453260647), (0, 554.0303071037565), (0, 536.9920349003851), (0, 506.0477664217294), (0, 473.15032669640556), (0, 442.9133493817657), (0, 416.8426971616819), (0, 394.971536332637), (0, 376.6316023563539), (0, 361.9439888953211), (0, 350.5793409455939), (0, 341.69562421389077), (0, 334.6086307253654), (0, 328.80720377167876), (0, 323.90249543812087), (0, 319.6254823198112), (0, 315.79626695921604), (0, 312.285196366728), (0, 308.99692324698793), (0, 305.8639148058771), (0, 302.8400283377445), (0, 299.8933337481722), (0, 297.0012215746866), (0, 294.15024004104055), (0, 291.33750174551204), (0, 288.56711873647095), (0, 285.84414817149826), (0, 283.1745036406096), (0, 280.56497423101524), (0, 278.01646296881836), (0, 275.52109320660156), (0, 273.0681384109265), (0, 270.64935506871166), (0, 268.25976980074495), (0, 265.8966174912632), (0, 263.5582286251528), (0, 261.24318453013757), (0, 258.94980704692824), (0, 256.67612942221956), (0, 254.4206105736279), (0, 252.18366973419697), (0, 249.968698927001), (0, 247.78015690545396), (0, 245.6198573410418), (0, 243.4861762793174), (0, 241.37637539016703), (0, 239.28832608796648), (0, 237.22073382048708), (0, 235.17285326570578), (0, 233.14424022191227), (0, 231.13461300326273), (0, 229.14378289114873), (0, 227.17161851088392), (0, 225.21802581048647), (0, 223.28293534146536), (0, 221.36629323562565), (0, 219.46805430624627), (0, 217.58817656713134), (0, 215.72661683359522), (0, 213.88332723041387), (0, 212.05825250166208), (0, 210.25132804625676), (0, 208.46247861283973), (0, 206.69161758852144), (0, 204.93864681290944), (0, 203.2034568444864), (0, 201.48592760238873), (0, 199.78592930408695), (0, 198.10332361928985), (0, 196.4379649631725), (0, 194.78970185809314), (0, 193.15837830211626), (0, 191.54383509412705), (0, 189.94591107777651), (0, 188.3644442784217), (0, 186.79927291717976), (0, 185.25023629344432), (0, 183.71717553165055), (0, 182.1999341904081), (0, 180.69835873320952), (0, 179.21229886050506), (0, 177.74160770304), (0, 176.28614187492568), (0, 174.84576137914772), (0, 173.4203293413936), (0, 172.00971150327), (0, 170.6137752849585), (0, 169.23238787396147), (0, 167.86541162637218), (0, 166.51269044959452), (0, 165.17399761551908), (0, 163.84875060768388), (0, 162.53371172828201), (0, 161.35176368121984), (0, 160.06693617675924), (0, 158.79559280116123), (0, 157.5376373476845), (0, 156.29297239160124), (0, 155.0614994235085), (0, 153.84311899434218)]