Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=4
Hidden channels=32
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=200

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=874.9651012420654
Loss over time=[(0, 16.97981526859039), (0, 17.109353292685697), (0, 16.063092720076945), (0, 15.915446613128402), (0, 16.21886950280036), (0, 16.216078461408205), (0, 16.180434060324487), (0, 16.302243283879545), (0, 16.428328865060504), (0, 16.46209413653457), (0, 16.432042363057107), (0, 16.415634309371914), (0, 16.443624843378633), (0, 16.49943463795911), (0, 16.60574337648255), (0, 16.750901934628878), (0, 16.801044964968487), (0, 16.69826192785294), (0, 16.544975081550213), (0, 16.431786544845995), (0, 16.364683196970198), (0, 16.32745918641819), (0, 16.328407812764496), (0, 16.366951605686285), (0, 16.390295917243243), (0, 16.348927782991314), (0, 16.269597115315804), (0, 16.203222085064425), (0, 16.161910428713206), (0, 16.139275460836032), (0, 16.135856579860963), (0, 16.142863184438188), (0, 16.132532912734867), (0, 16.088938727782605), (0, 16.03004962031234), (0, 15.982657220371662), (0, 15.957369933464243), (0, 15.954455143991062), (0, 15.97320199023216), (0, 16.00457152030382), (0, 16.026950774291358), (0, 16.026286015612378), (0, 16.01230781981132), (0, 16.004482678798443), (0, 16.014610793791636), (0, 16.045393573657122), (0, 16.09081545454036), (0, 16.132010056198204), (0, 16.14778910669804), (0, 16.137730924301177), (0, 16.120452179595077), (0, 16.11053574717122), (0, 16.109962234499946), (0, 16.112900581908214), (0, 16.107961156676645), (0, 16.08368230296331), (0, 16.039293434510128), (0, 15.985347312560084), (0, 15.934238388293716), (0, 15.890127891706843), (0, 15.845072544507248), (0, 15.793765958331633), (0, 15.741740069630136), (0, 15.6909653438922), (0, 15.64048612810352), (0, 15.589770876489231), (0, 15.535581758598557), (0, 15.48663426066338), (0, 15.452795758668522), (0, 15.428522047793908), (0, 15.40032277398455), (0, 15.360027642011389), (0, 15.323646493962107), (0, 15.30471014254634), (0, 15.299671131142354), (0, 15.291022777331118), (0, 15.273394583705597), (0, 15.258145425769396), (0, 15.252285885920411), (0, 15.250704026956509), (0, 15.251745306910877), (0, 15.253587452196552), (0, 15.250660153256607), (0, 15.242578292440687), (0, 15.241579747620769), (0, 15.24811324840126), (0, 15.248391521239194), (0, 15.239800997003233), (0, 15.232477936611247), (0, 15.227811726715357), (0, 15.222529272142879), (0, 15.21733307138254), (0, 15.206739343843896), (0, 15.190777421385182), (0, 15.180707743423566), (0, 15.1741493026421), (0, 15.161499147549346), (0, 15.147308120719394), (0, 15.13474063079214), (0, 15.123004370109262), (0, 15.115401609411737), (0, 15.10564088893475), (0, 15.091674753751747), (0, 15.083246724913963), (0, 15.0775619228235), (0, 15.069732030693958), (0, 15.062546709059589), (0, 15.054371672284555), (0, 15.048929670591967), (0, 15.046746194644921), (0, 15.039966527595375), (0, 15.033827147534385), (0, 15.030706621209532), (0, 15.026681846027222), (0, 15.023469764181863), (0, 15.017637426990278), (0, 15.012866179004293), (0, 15.011231663109653), (0, 15.005913260499678), (0, 15.001102597140388), (0, 14.996599978080354), (0, 14.992497894028478), (0, 14.989220918784437), (0, 14.982485011129699), (0, 14.978601066826615), (0, 14.974653483438237), (0, 14.969856600573724), (0, 14.965352470246458), (0, 14.960047595321196), (0, 14.957790256117724), (0, 14.95269507793192), (0, 14.949082882636972), (0, 14.945383390566333), (0, 14.942850386519995), (0, 14.939811334970472), (0, 14.936156066090334), (0, 14.934864449543605), (0, 14.931960115314766), (0, 14.93089419175038), (0, 14.927220640705933), (0, 14.928202130647882), (0, 14.923935408270374), (0, 14.926197820512922), (0, 14.919394633373674), (0, 14.927217395861486), (0, 14.913502188250398), (0, 14.933695658365135), (0, 14.903776353165291), (0, 14.960470913906988), (0, 14.898673986784729), (0, 15.000686062727654), (0, 14.895173628815423), (0, 14.936353855668752), (0, 14.920807069516199), (0, 14.878628137668699), (0, 14.961660345009262), (0, 14.883162404155534), (0, 14.877844238641638), (0, 14.93944854870917), (0, 14.877172071474941), (0, 14.909283265466076), (0, 14.915587619221213), (0, 14.867512194866197), (0, 14.93887399565814), (0, 14.908481732674584), (0, 14.873088983737361), (0, 14.945955803574748), (0, 14.911442266657424), (0, 14.905155632013251), (0, 14.944967148869718), (0, 14.903549739094139), (0, 14.94099613010285), (0, 14.941467573498159), (0, 14.898617329006791), (0, 14.954151104350917), (0, 14.93277073634432), (0, 14.91303233966421), (0, 14.94890584230756), (0, 14.914633877555312), (0, 14.938927560403862), (0, 14.934282954568452), (0, 14.905087003786257), (0, 14.951971540077889), (0, 14.921300321855647), (0, 14.924330350257092), (0, 14.940145921954365), (0, 14.918908081810658), (0, 14.95459519327131), (0, 14.923169721413533), (0, 14.9367844596096), (0, 14.952331585033132), (0, 14.933410728475124), (0, 14.959987583315549), (0, 14.934956940428885), (0, 14.967492323147837), (0, 14.948449607600434), (0, 14.957252610681104), (0, 14.96339279917277), (0, 14.959528670897546), (0, 14.97588488819045)]