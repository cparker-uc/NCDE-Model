Model Setup for {PATIENT_GROUPS} #{INDIVIDUAL_NUMBER} Trained Network:

RNN Network Architecture Parameters
Input channels=1
Hidden channels=256
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
Training Iterations=100
Learning rate=0.0001
Weight decay=0.0
Optimizer reset frequency=1000

Dropout probability (after initial linear layer before NCDE): 0.0
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Control']
Augmentation strategy=None
Noise Magnitude=0.0
Normalized/standardized=None
Number of virtual patients per real patient=0
Label smoothing factor=0
Test Patient Combinations:
Training batch size=1
Training Results:
Runtime=0.5028817653656006
Loss over time=[(0, 58.97314922387354), (0, 57.5075488313023), (0, 57.91493614132366), (0, 58.37048910093799), (0, 57.65566962200156), (0, 57.000429025133954), (0, 56.55184956261542), (0, 56.51543363018786), (0, 56.632489440807255), (0, 56.558998175269835), (0, 56.07004191205408), (0, 55.33845901849352), (0, 55.64828537037157), (0, 55.84922062836232), (0, 55.591907801244524), (0, 54.964300565350094), (0, 54.61433593890346), (0, 54.89655110358722), (0, 54.85577513111235), (0, 54.51455531946882), (0, 54.35320735037784), (0, 53.937970406187894), (0, 53.86605560245439), (0, 54.22325517314423), (0, 54.14788054410766), (0, 53.59614102256239), (0, 53.11587209419398), (0, 53.382515407955125), (0, 53.342470884067794), (0, 52.9916417776499), (0, 52.783640997419994), (0, 52.38644445045681), (0, 52.219950157797214), (0, 52.211969755774646), (0, 51.92247890101249), (0, 51.79914865739376), (0, 51.80275493070252), (0, 51.599433192512976), (0, 51.47741243887126), (0, 51.09117472109878), (0, 51.13692783165445), (0, 51.14841369756395), (0, 50.81300055666198), (0, 50.60959045621967), (0, 50.571491340279806), (0, 50.30853021100452), (0, 50.06924954133238), (0, 50.02831002671225), (0, 49.79501588279211), (0, 49.58943486582769), (0, 49.5459234457502), (0, 49.42301505906069), (0, 49.17867178726191), (0, 49.08014284205365), (0, 48.93185194741674), (0, 48.8731496199615), (0, 48.59092627526311), (0, 48.50906344862051), (0, 48.426197601123114), (0, 48.226670251979996), (0, 48.182601225049424), (0, 48.11770931098899), (0, 47.809793457436214), (0, 47.65357915942657), (0, 47.65311808967175), (0, 47.33188605578706), (0, 47.2439008310245), (0, 47.24626681352), (0, 46.90162157330408), (0, 46.85258541889539), (0, 46.85013372981266), (0, 46.56493722624752), (0, 46.269321819624594), (0, 46.29536665344928), (0, 46.06847986718569), (0, 45.863465331870266), (0, 45.94660900068877), (0, 45.860464633202), (0, 45.644694963544524), (0, 45.21551422420649), (0, 45.39208745433348), (0, 45.54201845416256), (0, 45.38136697686272), (0, 44.96251001639266), (0, 44.486064086269764), (0, 44.77397751444711), (0, 44.72273738800426), (0, 44.612569802015166), (0, 44.28997284074992), (0, 43.8195007877307), (0, 43.95494767804696), (0, 43.82702400479267), (0, 43.572212830927654), (0, 43.38991934270368), (0, 43.379342148455244), (0, 43.15557333916019), (0, 42.96161496661223), (0, 42.932058883752575), (0, 42.63744298711068), (0, 42.56872547031432)]