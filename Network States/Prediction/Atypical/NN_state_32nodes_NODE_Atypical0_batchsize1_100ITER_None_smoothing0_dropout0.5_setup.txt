Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=4
Hidden channels=32
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=200

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Atypical']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=174.77046608924866
Loss over time=[(0, 87.246195486117), (0, 80.24194771700621), (0, 75.91061602552419), (0, 72.7328953657162), (0, 70.82928965896112), (0, 69.84652025271058), (0, 69.37814675894231), (0, 69.08971611587936), (0, 68.8250327338425), (0, 68.55570355196646), (0, 68.31167528967268), (0, 68.13985351439432), (0, 68.0777550312782), (0, 68.13624311324293), (0, 68.28815396108632), (0, 68.47328600983893), (0, 68.62625541559981), (0, 68.70223803742103), (0, 68.68401550364305), (0, 68.57697837593749), (0, 68.40146505593017), (0, 68.18567450426667), (0, 67.95863045333398), (0, 67.74320200441392), (0, 67.5514879851243), (0, 67.385039474606), (0, 67.23935597596795), (0, 67.10936981272022), (0, 66.99300600790814), (0, 66.89198973298), (0, 66.81055348050081), (0, 66.7529274225122), (0, 66.72050135131836), (0, 66.70960391738927), (0, 66.71093663866313), (0, 66.71140763770386), (0, 66.6982797734686), (0, 66.66402662842384), (0, 66.6090559305371), (0, 66.54035815858695), (0, 66.46708151118688), (0, 66.39616921719282), (0, 66.33055799329152), (0, 66.27002259128369), (0, 66.21298158016913), (0, 66.15777497827358), (0, 66.10307399213961), (0, 66.04774969093198), (0, 65.99057656687563), (0, 65.93003355372048), (0, 65.86435757420912), (0, 65.79191905657999), (0, 65.71183296670293), (0, 65.62455872412224), (0, 65.53212332389977), (0, 65.43774374897816), (0, 65.34497417800813), (0, 65.25682122516872), (0, 65.17523794834585), (0, 65.10104080214926), (0, 65.0339698841143), (0, 64.9727374023314), (0, 64.91535443068082), (0, 64.85995774613451), (0, 64.80575028265459), (0, 64.75322348052114), (0, 64.70346044783793), (0, 64.6572963811919), (0, 64.61503225150278), (0, 64.57648641547611), (0, 64.54091191801963), (0, 64.50703222286515), (0, 64.4737295890969), (0, 64.44092233751823), (0, 64.40957893195437), (0, 64.38080511312351), (0, 64.3550287035349), (0, 64.33182496775464), (0, 64.31005102357796), (0, 64.28823106120981), (0, 64.26537176480477), (0, 64.24171092219954), (0, 64.21859265289373), (0, 64.19759571887506), (0, 64.17977279355556), (0, 64.16541069691287), (0, 64.15410079121563), (0, 64.14511276831004), (0, 64.13800948172158), (0, 64.13304002632195), (0, 64.13088894606824), (0, 64.13213423396502), (0, 64.13689825696036), (0, 64.1448042574833), (0, 64.15516681845696), (0, 64.16736694664863), (0, 64.1812116208196), (0, 64.1969039854591), (0, 64.21464648988768), (0, 64.23425114235879)]