Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=4
Hidden channels=32
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=100
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=200

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Atypical']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=159.77785897254944
Loss over time=[(0, 105.76639268612468), (0, 93.48158873441109), (0, 83.5037084310303), (0, 76.67259428010838), (0, 72.81761300642926), (0, 70.31755333834451), (0, 68.62453446517688), (0, 67.37930038227756), (0, 66.40633292880297), (0, 65.67514209068234), (0, 65.1729843865917), (0, 64.84840212633415), (0, 64.65221988285559), (0, 64.55509590024396), (0, 64.52752334864589), (0, 64.52631068511249), (0, 64.51371157762762), (0, 64.484470829516), (0, 64.45922343692632), (0, 64.45454561274262), (0, 64.46947503164185), (0, 64.49401696755189), (0, 64.51769799818152), (0, 64.53085955711347), (0, 64.52663447205373), (0, 64.50668084893658), (0, 64.48207070540771), (0, 64.46494796292527), (0, 64.46020737642854), (0, 64.46493846846722), (0, 64.47311320387033), (0, 64.47997003587153), (0, 64.48369856853202), (0, 64.48509132471767), (0, 64.4864964795914), (0, 64.49077179893708), (0, 64.49988512420705), (0, 64.51224378458225), (0, 64.52021345733182), (0, 64.51398015664373), (0, 64.49111812226423), (0, 64.45870854041407), (0, 64.4248788587174), (0, 64.39278524372074), (0, 64.36322596264694), (0, 64.33922445274362), (0, 64.32693826804584), (0, 64.33309623012539), (0, 64.36059749264801), (0, 64.40420505821638), (0, 64.45121600693997), (0, 64.48985371599463), (0, 64.51753010809809), (0, 64.53923482117949), (0, 64.55965159473257), (0, 64.57902294461776), (0, 64.5943409107969), (0, 64.60227890940654), (0, 64.60322616914158), (0, 64.60274268917625), (0, 64.60625234986666), (0, 64.61409195456156), (0, 64.62249025100967), (0, 64.6274237460022), (0, 64.62945039663713), (0, 64.63290694328964), (0, 64.63972647726656), (0, 64.64691956826611), (0, 64.64893625299314), (0, 64.64316536614663), (0, 64.63223026352523), (0, 64.61948886248732), (0, 64.60560926316171), (0, 64.58960286564283), (0, 64.57278987817388), (0, 64.56010303664084), (0, 64.55535588705033), (0, 64.55762438950316), (0, 64.56263458695724), (0, 64.56730927644867), (0, 64.57164305054036), (0, 64.57586856426401), (0, 64.57869047011513), (0, 64.57884698812383), (0, 64.57787355605312), (0, 64.57950281755751), (0, 64.58594317560944), (0, 64.59620995009902), (0, 64.60805203972035), (0, 64.62077541279005), (0, 64.63507179435196), (0, 64.6509691555149), (0, 64.66745566916988), (0, 64.68405597533079), (0, 64.70181723473164), (0, 64.72180392541094), (0, 64.74334870760691), (0, 64.7645135455126), (0, 64.784069284986), (0, 64.80240603071627)]