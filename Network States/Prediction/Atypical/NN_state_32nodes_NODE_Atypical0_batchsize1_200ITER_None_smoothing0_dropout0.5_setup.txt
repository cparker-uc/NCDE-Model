Model Setup for {PATIENT_GROUPS} Trained Network:

{NETWORK_TYPE} Network Architecture Parameters
Input channels=4
Hidden channels=32
Output channels=4

Training hyperparameters
Optimizer=AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)Training Iterations=200
Learning rate=0.001
Weight decay=0.0
Optimizer reset frequency=200

Dropout probability (after initial linear layer before NCDE): 0.5
Training Data Selection Parameters
(If not virtual, the only important params are the groups and whether data was normalized/standardized)
Patient groups=['Atypical']
Augmentation strategy=Uniform
Noise Magnitude=0.05
Normalized/standardized=None
Number of virtual patients per real patient=100
Label smoothing factor=0
Test Patient Combinations:
Control: None
MDD: None
Training batch size=1
Training Results:
Runtime=352.13602805137634
Loss over time=[(0, 87.246195486117), (0, 80.24194771700621), (0, 75.91061602552419), (0, 72.7328953657162), (0, 70.82928965896112), (0, 69.84652025271058), (0, 69.37814675894231), (0, 69.08971611587936), (0, 68.8250327338425), (0, 68.55570355196646), (0, 68.31167528967268), (0, 68.13985351439432), (0, 68.0777550312782), (0, 68.13624311324293), (0, 68.28815396108632), (0, 68.47328600983893), (0, 68.62625541559981), (0, 68.70223803742103), (0, 68.68401550364305), (0, 68.57697837593749), (0, 68.40146505593017), (0, 68.18567450426667), (0, 67.95863045333398), (0, 67.74320200441392), (0, 67.5514879851243), (0, 67.385039474606), (0, 67.23935597596795), (0, 67.10936981272022), (0, 66.99300600790814), (0, 66.89198973298), (0, 66.81055348050081), (0, 66.7529274225122), (0, 66.72050135131836), (0, 66.70960391738927), (0, 66.71093663866313), (0, 66.71140763770386), (0, 66.6982797734686), (0, 66.66402662842384), (0, 66.6090559305371), (0, 66.54035815858695), (0, 66.46708151118688), (0, 66.39616921719282), (0, 66.33055799329152), (0, 66.27002259128369), (0, 66.21298158016913), (0, 66.15777497827358), (0, 66.10307399213961), (0, 66.04774969093198), (0, 65.99057656687563), (0, 65.93003355372048), (0, 65.86435757420912), (0, 65.79191905657999), (0, 65.71183296670293), (0, 65.62455872412224), (0, 65.53212332389977), (0, 65.43774374897816), (0, 65.34497417800813), (0, 65.25682122516872), (0, 65.17523794834585), (0, 65.10104080214926), (0, 65.0339698841143), (0, 64.9727374023314), (0, 64.91535443068082), (0, 64.85995774613451), (0, 64.80575028265459), (0, 64.75322348052114), (0, 64.70346044783793), (0, 64.6572963811919), (0, 64.61503225150278), (0, 64.57648641547611), (0, 64.54091191801963), (0, 64.50703222286515), (0, 64.4737295890969), (0, 64.44092233751823), (0, 64.40957893195437), (0, 64.38080511312351), (0, 64.3550287035349), (0, 64.33182496775464), (0, 64.31005102357796), (0, 64.28823106120981), (0, 64.26537176480477), (0, 64.24171092219954), (0, 64.21859265289373), (0, 64.19759571887506), (0, 64.17977279355556), (0, 64.16541069691287), (0, 64.15410079121563), (0, 64.14511276831004), (0, 64.13800948172158), (0, 64.13304002632195), (0, 64.13088894606824), (0, 64.13213423396502), (0, 64.13689825696036), (0, 64.1448042574833), (0, 64.15516681845696), (0, 64.16736694664863), (0, 64.1812116208196), (0, 64.1969039854591), (0, 64.21464648988768), (0, 64.23425114235879), (0, 64.25501877228712), (0, 64.27592340892748), (0, 64.29600378722101), (0, 64.31476417492438), (0, 64.33230149272606), (0, 64.34903452928565), (0, 64.36531779860906), (0, 64.38120438390806), (0, 64.39645215099058), (0, 64.4107383706112), (0, 64.4239342890651), (0, 64.43623944375459), (0, 64.44806235089938), (0, 64.45978076863402), (0, 64.47158727500894), (0, 64.48348902951003), (0, 64.49545655521437), (0, 64.50759910246953), (0, 64.52020698809302), (0, 64.53362137543856), (0, 64.54803212814421), (0, 64.56336286468796), (0, 64.57932027865536), (0, 64.59557483719345), (0, 64.61194950994978), (0, 64.62847952870717), (0, 64.64531625298757), (0, 64.66256526984904), (0, 64.68018428124576), (0, 64.69800682822972), (0, 64.71585423576016), (0, 64.7336362081456), (0, 64.75137515445076), (0, 64.76913881595416), (0, 64.78694749510615), (0, 64.80475535241013), (0, 64.8224945346825), (0, 64.84016475014602), (0, 64.85785876139146), (0, 64.87572062992439), (0, 64.89384286061735), (0, 64.91221665640373), (0, 64.93072937266739), (0, 64.94925272407471), (0, 64.96771477172508), (0, 64.98613487834248), (0, 65.00457231613137), (0, 65.02306581035359), (0, 65.04159544142283), (0, 65.06009915744238), (0, 65.07852231925077), (0, 65.09685197537596), (0, 65.11511561305562), (0, 65.1333330977319), (0, 65.1514976473366), (0, 65.1695841789415), (0, 65.18757580683311), (0, 65.2055072923226), (0, 65.22343559586834), (0, 65.24140702042428), (0, 65.25941626837255), (0, 65.27740205813662), (0, 65.29527424728593), (0, 65.312955832348), (0, 65.3304012319801), (0, 65.3475892259245), (0, 65.3645037223127), (0, 65.38111025931106), (0, 65.39736411327812), (0, 65.41323201664974), (0, 65.42870536155728), (0, 65.44378921929763), (0, 65.45849782559222), (0, 65.4728412097057), (0, 65.48683036507438), (0, 65.5004991282401), (0, 65.51390079788158), (0, 65.52711101622124), (0, 65.54020089922784), (0, 65.55321802688475), (0, 65.56619360480566), (0, 65.57914642740978), (0, 65.59209972273312), (0, 65.60508796178104), (0, 65.61814921512513), (0, 65.63130841043755), (0, 65.6445843040272), (0, 65.6579841822686), (0, 65.67151810596846), (0, 65.68519699012857), (0, 65.69902697715), (0, 65.71300528008946), (0, 65.72712594925473), (0, 65.74138456469244), (0, 65.75578151398453), (0, 65.77032718147606), (0, 65.78503291673128), (0, 65.79989339897668), (0, 65.8149015543967), (0, 65.83004034049739)]